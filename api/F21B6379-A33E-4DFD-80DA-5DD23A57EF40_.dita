<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "file:///C:/InfoShare/Web/Author/ASP/DocTypes/dita-oasis/1.2/technicalContent/dtd/reference.dtd">
<reference id="GUID-F21B6379-A33E-4DFD-80DA-5DD23A57EF40" xml:lang="en" xmlns:ditaarch="http://dita.oasis-open.org/architecture/2005/">
  <title id="GUID-9E5A70B1-D45E-4B3F-BEB4-DDD6B1EBF70E">Cluster fault codes</title>
  <shortdesc>If a storage cluster experiences an error or a state that might be of interest to an administrator, it generates a cluster fault. You can use the <apiname>ListClusterFaults</apiname> method to retrieve the current list of resolved and unresolved faults on a storage cluster.</shortdesc>
  <prolog>
    <metadata>
      <keywords>
        <keyword>cluster</keyword>
        <keyword>fault</keyword>
        <keyword>codes</keyword>
        <keyword>code</keyword>
        <keyword>message</keyword>
        <keyword>alert</keyword>
        <keyword>error</keyword>
      </keywords>
    </metadata>
  </prolog>
  <refbody>
    <section id="GUID-3BF4DF5C-0AEF-4DEC-B12F-E23499C27429">
      <p>The following list gives more information about and possible solutions for NetApp Element storage cluster faults:</p>
      <dl>
        <dlentry id="GUID-D9A2FB33-FA3F-47E6-AB8F-83AA97728DB5">
          <dt>authenticationServiceFault</dt>
          <dd>The Authentication Service on one or more cluster nodes is not functioning as expected.</dd>
          <dd>Contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-BAF959D7-C842-4568-A85E-98B5024BBC7E">
          <dt>availableVirtualNetworkIPAddressesLow</dt>
          <dd>The number of virtual network addresses in the block of IP addresses is low.   </dd>
          <dd>To resolve this fault, add more IP addresses to the block of virtual network addresses.</dd>
        </dlentry>
        <dlentry id="GUID-CDE93C32-14B3-4A12-B5FC-10C49B28ACE2">
          <dt>blockClusterFull</dt>
          <dd>There is not enough free block storage space to support a single node loss. See the <apiname>GetClusterFullThreshold</apiname> API method for details on cluster fullness levels. This cluster fault indicates one of the following conditions:<ul id="GUID-6D17ECAB-6075-4634-939E-8001530FEE84"><li>stage3Low (Warning): User-defined threshold was crossed. Adjust Cluster Full settings or add more nodes.</li><li>stage4Critical (Error): There is not enough space to recover from a 1-node failure. Creation of volumes, snapshots, and clones is not allowed.</li><li>stage5CompletelyConsumed (Critical)1; No writes or new iSCSI connections are allowed. Current iSCSI connections will be maintained. Writes will fail until more capacity is added to the cluster.</li></ul></dd>
          <dd>To resolve this fault, purge or delete volumes or add another storage node to the storage cluster. </dd>
        </dlentry>
        <dlentry id="GUID-3D1109AA-6371-4BCB-96DA-B715B2C38B0E">
          <dt>blocksDegraded</dt>
          <dd>Block data is no longer fully replicated due to a failure. </dd>
          <dd>
            <simpletable frame="all" relcolwidth="1.0* 1.0*">
              <sthead>
                <stentry>Severity</stentry>
                <stentry>Description</stentry>
              </sthead>
              <strow>
                <stentry>Warning</stentry>
                <stentry>Only two complete copies of the block data are accessible.</stentry>
              </strow>
              <strow>
                <stentry>Error</stentry>
                <stentry>Only a single complete copy of the block data is accessible.</stentry>
              </strow>
              <strow>
                <stentry>Critical</stentry>
                <stentry>No complete copies of the block data are accessible.</stentry>
              </strow>
            </simpletable>
            <note id="GUID-EE1EEB6B-4019-42B4-8CBB-FE5101078B37">The warning status can only occur on a Triple Helix system.</note>
          </dd>
          <dd>To resolve this fault, restore any offline nodes or block services, or contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-3A8A9CFC-8149-4917-8B4E-8F1B9428A96F">
          <dt>blockServiceTooFull</dt>
          <dd>A block service is using too much space.</dd>
          <dd>To resolve this fault, purge or delete volumes or add another storage node to the storage cluster.</dd>
        </dlentry>
        <dlentry id="GUID-2967CF6E-A1C7-490E-8AFF-DE752E655A6C">
          <dt>clockSkewExceedsFaultThreshold</dt>
          <dd>The time skew between the cluster master and the node which is presenting a token exceeds the recommended threshold.</dd>
          <dd>The storage cluster cannot correct the time skew between the nodes automatically. To resolve this fault, use NTP servers that are internal to your network, rather than the installation defaults. If you are already using internal NTP servers, contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-1BCF3AF2-6F9B-4187-9F73-0F17AFA8EF51">
          <dt>clusterCannotSync</dt>
          <dd>Cluster block data is in a degraded state and the auto-heal process to restore full block data redundancy cannot proceed; either too many nodes or block services are offline, or the cluster block services are too full.</dd>
          <dd>To resolve this fault, add more block capacity or contact NetApp Support.</dd>
        </dlentry>
        <dlentry id="GUID-AC3E6297-26A7-4D40-A041-C8F9E2FD0926">
          <dt>clusterFull</dt>
          <dd>There is no more free storage space in the storage cluster.</dd>
          <dd>To resolve this fault, add more storage.</dd>
        </dlentry>
        <dlentry id="GUID-18AD8146-EFFC-4C68-A90D-1D212BA1FA0D">
          <dt>clusterIOPSAreOverProvisioned</dt>
          <dd>Storage cluster IOPS are over provisioned. The sum of all minimum QoS IOPS is greater than the expected IOPS of the cluster. The system cannot maintain minimum QoS for all volumes simultaneously.</dd>
          <dd>To resolve this fault, lower the minimum QoS IOPS settings for volumes.</dd>
        </dlentry>
        <dlentry id="GUID-FD7EF72B-0070-49B0-B302-DA7498731489">
          <dt>disableDriveSecurityFailed</dt>
          <dd>A drive was unable to be secure disabled when the Encryption at Rest feature is turned off. The drive still has drive security enabled.</dd>
          <dd>The reason that drive security could not be disabled is shown in the fault details; you might need to investigate the problem based on the reason. If you need to recover a disk that does not successfully disable security, use the following steps:<ol id="GUID-4CA5AE15-9CAC-4646-9B3F-3294BF673732"><li>Logically remove the drive by moving it to "available" status.</li><li>Perform a secure erase on the drive.</li><li>Move the drive to "active" status.</li></ol>If these steps do not resolve the issue, replace the drive.</dd>
        </dlentry>
        <dlentry id="GUID-3ABF9F75-8C04-48B4-B780-B46DC8027A9E">
          <dt>disconnectedClusterPair</dt>
          <dd>A cluster pair is disconnected or configured incorrectly.</dd>
          <dd>Check the network connectivity of the cluster.</dd>
        </dlentry>
        <dlentry id="GUID-86B6B1F3-AE93-45E8-947C-4BC360FC683C">
          <dt>disconnectedRemoteNode</dt>
          <dd>A remote node is disconnected or configured incorrectly. Check network connectivity between the nodes.</dd>
        </dlentry>
        <dlentry id="GUID-D47822E2-CB1A-4423-85E8-DACDBCCE9331">
          <dt>disconnectedSnapMirrorEndpoint</dt>
          <dd>A remote SnapMirror endpoint is disconnected or configured incorrectly. Check network connectivity between the cluster and the remote SnapMirrorEndpoint.</dd>
        </dlentry>
        <dlentry id="GUID-C52F36AA-371C-4FAC-A1F2-1BF6EA504A00">
          <dt>driveAvailable</dt>
          <dd>One or more drives are available to be added in the storage cluster. In general, all storage clusters should have all drives added and none in the available state. If this fault appears unexpectedly, contact  NetApp Support.</dd>
          <dd>To resolve this fault, add any available drives to the storage cluster.</dd>
        </dlentry>
        <dlentry id="GUID-81705FFC-8269-42AF-9C5A-29773673E206">
          <dt>driveFailed</dt>
          <dd>The cluster returns this fault when one or more drives have failed, indicating one of the following conditions:<ul id="GUID-5447F1BB-917D-49DF-B498-3CEF37E04302"><li>The drive manager cannot access the drive.</li><li>The slice or block service has failed too many times, presumably because of drive read or write failures, and cannot restart.</li><li>The drive is missing.</li><li>The master service for the node is inaccessible (all drives in the node are considered missing/failed).</li><li>The drive is locked and the authentication key for the drive cannot be acquired.</li><li>The drive is locked and the unlock operation fails.</li></ul>To resolve this issue:<ul id="GUID-10F0945B-FB4C-4624-8F53-5C1D0C7D0C96"><li>Check network connectivity for the node.</li><li>Replace the drive.</li><li>Ensure that the authentication key is available.</li></ul></dd>
        </dlentry>
        <dlentry id="GUID-EA2DEA4A-49EF-4458-9112-8BFCCA7AB9AB">
          <dt>driveHealthFault</dt>
          <dd>A drive has failed the SMART health check and as a result, the drive’s functions are diminished. There is a Critical severity level for this fault:<ul id="GUID-EAAE3DF8-2113-473E-AAAB-253F6ED95620"><li>Drive with serial: &lt;serial number&gt; in slot: &lt;node slot&gt;&lt;drive slot&gt; has failed the SMART overall health check.</li></ul>To resolve this fault, replace the drive.</dd>
        </dlentry>
        <dlentry id="GUID-0ECB89CE-79F6-4E6D-A14C-BCADAA79ACB1">
          <dt>driveWearFault</dt>
          <dd>A drive's remaining life has dropped below thresholds, but it is still functioning.There are two possible severity levels for this fault: Critical and Warning:<ul id="GUID-F00C65C5-A6A4-4E13-9B2F-43BD11871727"><li>Drive with serial: &lt;serial number&gt; in slot: &lt;node slot&gt;&lt;drive slot&gt; has critical wear levels.</li><li>Drive with serial: &lt;serial number&gt; in slot: &lt;node slot&gt;&lt;drive slot&gt; has low wear reserves.</li></ul>To resolve this fault, replace the drive soon.</dd>
        </dlentry>
        <dlentry id="GUID-881C95C7-2FB6-424C-BBBF-4C7CA8B47511">
          <dt>duplicateClusterMasterCandidates</dt>
          <dd>There is more than one storage cluster master candidate.</dd>
          <dd>Contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-1830D2F2-E801-4107-962C-D4D5E45BE9F9">
          <dt>enableDriveSecurityFailed</dt>
          <dd>A drive was unable to be secure enabled when the Encryption at Rest feature is turned on.</dd>
          <dd>Ensure that the correct key is being used to enable security. If you need to recover a disk that does not successfully enable security, use the following steps:<ol id="GUID-E0490503-0FC1-41A2-B061-469A6D8CE238"><li>Logically remove the drive by moving it to "available" status.</li><li>Perform a secure erase on the drive.</li><li>Move the drive to "active" status.</li></ol>If these steps do not resolve the issue, replace the drive.</dd>
        </dlentry>
        <dlentry id="GUID-2A31C6E4-060A-4CFF-8556-CB8F931E871B">
          <dt>ensembleDegraded</dt>
          <dd>One of the ensemble nodes has lost network connectivity or power.</dd>
          <dd>To resolve this fault, restore network connectivity or power to the affected node.</dd>
        </dlentry>
        <dlentry id="GUID-A0023609-D2E8-486B-A614-09D8D149A0A8">
          <dt>exception</dt>
          <dd>An unusual fault has occurred. These faults are not automatically cleared from the fault queue.</dd>
          <dd>Contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-7F245AF6-83AB-483F-957C-49B429E58CFD">
          <dt>failedSpaceTooFull</dt>
          <dd>A block service is not responding to data write requests. This causes the slice service to run out of space to store failed writes.</dd>
          <dd>Contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-5D81E51F-0BE4-4929-AC9B-158851D7F98E">
          <dt>fanSensor</dt>
          <dd>A fan sensor has failed or is missing.</dd>
          <dd>Replace any failed hardware in the node. If this does not resolve the issue, contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-43C1FAD8-0DD4-456B-B7D3-BBA26B171283">
          <dt>fibreChannelAccessDegraded</dt>
          <dd>A Fibre Channel node is not responding to other nodes in the storage cluster via its storage IP address.</dd>
          <dd>Check the network connectivity of the cluster.</dd>
        </dlentry>
        <dlentry id="GUID-6AE72876-B05F-4FAE-AFF5-302EC385DD1B">
          <dt>fibreChannelAccessUnavailable</dt>
          <dd>All  Fibre Channel nodes are unresponsive.  The  node IDs are displayed.</dd>
          <dd>Check the network connectivity of the cluster.</dd>
        </dlentry>
        <dlentry id="GUID-5529D14A-B87B-45A6-8A65-444EE983CF2E">
          <dt>fibreChannelActiveIxL</dt>
          <dd>The IxL Nexus count is approaching the supported limit of 8000 active sessions per Fibre Channel node.<ul id="GUID-5309645D-1EE7-47CD-BA49-471249C36C0F"><li>Best practice limit is 5500.</li><li>Warning limit is 7500.</li><li>Maximum limit (not enforced) is 8192.</li></ul></dd>
          <dd>To resolve this fault, reduce the IxL Nexus count below the best practice limit of 5500.</dd>
        </dlentry>
        <dlentry id="GUID-9E878396-A3F4-49CD-ACDA-8E6568B8BD5B">
          <dt>fibreChannelConfig</dt>
          <dd>This cluster fault indicates one of the following conditions:<ul id="UL_EE5B03C8A211435DA5EB65B55CF85987"><li id="LI_B11E77160EF945F4A2052DC748EB2619">There is an unexpected Fibre Channel port on a PCI slot.</li><li id="LI_7D534B1D42B54C65870C5E8D7E3D5F0D">There is an unexpected Fibre Channel HBA model.</li><li id="LI_DED3152C24DD467FB6426CC162A4AC0B">There is a problem with the firmware of a Fibre Channel HBA.</li><li id="LI_CB82DEAB54B04EC5A45B60FE1D0BBB5A">A Fibre Channel port is not online.</li><li id="LI_CB297FDF8937404E9241C86899B8D863">There is a persistent issue configuring Fibre Channel passthrough.</li></ul></dd>
          <dd>Contact NetApp Support for assistance. </dd>
        </dlentry>
        <dlentry id="GUID-EE783007-2ED4-4F19-8C5C-C2529405B1E7">
          <dt>fibreChannelIOPS</dt>
          <dd>The total IOPS count is approaching the IOPS limit for Fibre Channel nodes in the cluster. The limits are:<ul id="GUID-9B7DC4AE-7110-4A31-BBA9-17DE8A0734EC"><li>FC0025: 450K IOPS limit at 4K block size per Fibre Channel node.</li><li>FCN001: 625K OPS limit at 4K block size per Fibre Channel node.</li></ul></dd>
          <dd>To resolve this fault, balance the load across all available Fibre Channel nodes.</dd>
        </dlentry>
        <dlentry id="GUID-3A034470-320E-4D4F-8406-9E7A84400BA1">
          <dt>fibreChannelStaticIxL</dt>
          <dd>The IxL Nexus count is approaching the supported limit of 16000 static sessions per Fibre Channel node.<ul id="GUID-A560B710-1850-4B12-A883-1E2C9A27ED43"><li>Best practice limit is 11000.</li><li>Warning limit is 15000.</li><li>Maximum limit (enforced) is 16384.</li></ul></dd>
          <dd>To resolve this fault, reduce the IxL Nexus count below the best practice limit of 11000.</dd>
        </dlentry>
        <dlentry id="GUID-23123C7D-2C22-4894-BBA9-2E11C299E10C">
          <dt>fileSystemCapacityLow</dt>
          <dd>There is insufficient space on one of the filesystems.</dd>
          <dd>To resolve this fault, add more capacity to the filesystem.</dd>
        </dlentry>
        <dlentry id="GUID-4FA8490A-37AC-4524-B68C-458F2E5127C4">
          <dt>fipsDrivesMismatch</dt>
          <dd>A non-FIPS drive has been inserted into a FIPS storage node or a FIPS drive has been inserted into a non-FIPS storage node.</dd>
          <dd>Remove or replace the drive or drives in question.</dd>
        </dlentry>
        <dlentry id="GUID-559B6694-6ACF-483F-B049-AC11C41576BB">
          <dt>fipsDrivesOutOfCompliance</dt>
          <dd>The system has detected that Encryption at Rest is disabled, or non-FIPS hardware is present in the storage cluster.</dd>
          <dd>Enable Encryption at Rest or remove the non-FIPS hardware from the storage cluster.</dd>
        </dlentry>
        <dlentry id="GUID-C2C2690E-191C-44A9-9525-1EE533130155">
          <dt>fipsSelfTestFailure</dt>
          <dd>The system has detected a failure during the FIPS self test.</dd>
          <dd>Contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-E6FCFAC2-D251-4110-ABC3-A3CD4351E180">
          <dt>hardwareConfigMismatch</dt>
          <dd>This cluster fault indicates one of the following conditions:<ul id="UL_74A5AF07ED474FE98C40B76144CACE5F"><li id="LI_38E95FB1D2EE4F4396B3C5A03DB4FFAD">The configuration does not match the node definition.</li><li id="LI_C960B9C6BCEA43339F0AC000A9DFC70F">There is an incorrect drive size for this type of node.</li><li id="LI_371B5E5DE57D4A1685F4989BA195155A">A node is using unsupported drive.</li><li id="LI_D2027671A8E045F4BD6A8D0C8EBB27FF">There is a drive firmware mismatch.</li><li id="LI_053E397C68E14E1091182C98FAD75480">A drive's encryption capable state does not match its parent node.</li></ul></dd>
          <dd>Contact NetApp Support for assistance. </dd>
        </dlentry>
        <dlentry id="GUID-7F89CAA9-D86B-46DD-9474-A13F45A55E9F">
          <dt>idPCertificateExpiration</dt>
          <dd>The cluster’s service provider SSL certificate for use with a third-party identity provider is nearing expiration or has already expired. This fault uses the following severities based on urgency:<simpletable frame="all" relcolwidth="1.0* 1.0*"><sthead><stentry>Severity</stentry><stentry>Description</stentry></sthead><strow><stentry>Warning</stentry><stentry>Certificate expires within 30 days.</stentry></strow><strow><stentry>Error</stentry><stentry>Certificate expires within 7 days.</stentry></strow><strow><stentry>Critical</stentry><stentry>Certificate expires within 3 days or has already expired.</stentry></strow></simpletable></dd>
          <dd>To resolve this fault, update the SSL certificate before it expires. Use the <apiname>UpdateIdpConfiguration</apiname> method with <codeph>refreshCertificateExpirationTime=true</codeph> to provide the updated SSL certificate.</dd>
        </dlentry>
        <dlentry id="GUID-8A54E33B-B60E-4263-B009-3DB5E0A2A462">
          <dt>inconsistentBondModes</dt>
          <dd>The bond modes on the VLAN device are missing. This fault will display the expected bond mode and the bond mode currently in use.</dd>
          <dd>To resolve this fault, modify the bond modes in the per-node web UI.</dd>
        </dlentry>
        <dlentry id="GUID-30540E6A-D258-425D-921F-D2C6076DAC32">
          <dt>inconsistentInterfaceConfiguration</dt>
          <dd>The interface configuration is inconsistent.</dd>
          <dd>To resolve this fault, ensure the node interfaces in the storage cluster are consistently configured.</dd>
        </dlentry>
        <dlentry id="GUID-86C5257A-7B03-4D97-B1D8-A09C124DCFDA">
          <dt>inconsistentMtus</dt>
          <dd>This cluster fault indicates one of the following conditions:<ul id="UL_0124B4F1E800462DA89C7D215EF54C6B"><li id="LI_4F4CC1828DB2463BBBF10D2EE8F72FF4">Bond1G mismatch: Inconsistent MTUs have been detected on Bond1G interfaces. </li><li id="LI_7009F77E3E2B40E8856248C92360235B">Bond10G mismatch: Inconsistent MTUs have been detected on Bond10G interfaces. </li></ul></dd>
          <dd>This fault displays the node or nodes in question along with the associated MTU value.</dd>
          <dd>To resolve this fault, modify the MTU settings in the per-node web UI.</dd>
        </dlentry>
        <dlentry id="GUID-8199F273-5CD1-421B-9DEF-B2C65ACB7E82">
          <dt>inconsistentRoutingRules</dt>
          <dd>The routing rules for this interface are inconsistent.</dd>
        </dlentry>
        <dlentry id="GUID-FAD1CC66-E6D8-4A8F-9259-799C8C900FC5">
          <dt>inconsistentSubnetMasks</dt>
          <dd>The network mask on the VLAN device does not match the internally recorded network mask  for the VLAN. This fault displays the expected network mask and the network mask currently in use.</dd>
          <dd>To resolve this fault, modify the subnet mask in the Element (storage cluster) web UI.  </dd>
        </dlentry>
        <dlentry id="GUID-13823D60-DDC6-4E8C-BAD1-E0C75AD675C1">
          <dt>incorrectBondPortCount</dt>
          <dd>The number of bond ports is incorrect.</dd>
        </dlentry>
        <dlentry id="GUID-A77E83E0-4DA9-47F3-9350-A0FB6C2C8059">
          <dt>invalidConfiguredFibreChannelNodeCount</dt>
          <dd>One of the two expected  Fibre Channel node connections is degraded. This fault appears when only one  Fibre Channel node is connected.</dd>
          <dd>To resolve this fault, check the cluster network connectivity and network cabling, and check for failed services. If there are no network or service problems, contact NetApp Support for a Fibre Channel node replacement.  </dd>
        </dlentry>
        <dlentry id="GUID-A12E8258-846C-40ED-8F2B-6D112B5CAFFC">
          <dt>irqBalanceFailed</dt>
          <dd>An exception occurred while attempting to balance interrupts.</dd>
          <dd>Contact NetApp Support for assistance. </dd>
        </dlentry>
        <dlentry id="GUID-087719AA-14C0-4093-A8AC-6D3CCA09384D">
          <dt>kmipCertificateFault (Root Certification Authority (CA) certificate is nearing expiration)</dt>
          <dd>The root Certification Authority (CA) certificate is nearing expiration. This fault uses the following severities based on urgency:<simpletable frame="all" relcolwidth="1.0* 1.0*"><sthead><stentry>Severity</stentry><stentry>Description</stentry></sthead><strow><stentry>Warning</stentry><stentry>Certificate expires within 30 days.</stentry></strow><strow><stentry>Error</stentry><stentry>Certificate expires within 7 days.</stentry></strow><strow><stentry>Critical</stentry><stentry>Certificate expires within 3 days.</stentry></strow></simpletable></dd>
          <dd>To resolve this fault, update the certificate before it expires. Acquire a new certificate from the root CA with expiration date at least 30 days in the future. Use the <apiname>ModifyKeyServerKmip</apiname> API method to provide the updated root CA certificate.</dd>
        </dlentry>
        <dlentry id="GUID-996C05A1-5D0E-415B-AC96-AA7D2196C85F">
          <dt>kmipCertificateFault (Client certificate is nearing expiration)</dt>
          <dd>The client certificate is nearing expiration. This fault uses the following severities based on urgency:<simpletable frame="all" relcolwidth="1.0* 1.0*"><sthead><stentry>Severity</stentry><stentry>Description</stentry></sthead><strow><stentry>Warning</stentry><stentry>Certificate expires within 30 days.</stentry></strow><strow><stentry>Error</stentry><stentry>Certificate expires within 7 days.</stentry></strow><strow><stentry>Critical</stentry><stentry>Certificate expires within 3 days.</stentry></strow></simpletable></dd>
          <dd>To resolve this fault, create a new CSR with the <apiname>GetClientCertificateSigningRequest</apiname> method. Have the CSR signed with an expiration greater than 30 days and then use the <apiname>ModifyKeyServerKmip</apiname> API method to replace the expiring KMIP client certificate with the new certificate.</dd>
        </dlentry>
        <dlentry id="GUID-EFFA6BE5-2EAC-452B-9D2F-22A21B212934">
          <dt>kmipCertificateFault (Root Certification Authority (CA) certificate expired)</dt>
          <dd>The root CA certificate has expired.</dd>
          <dd>Acquire a new certificate from the root CA with expiration date at least 30 days in the future. Use the <apiname>ModifyKeyServerKmip</apiname> API method to provide the updated root CA certificate.</dd>
        </dlentry>
        <dlentry id="GUID-C5110F3A-EFF3-4900-B9C8-0F6C25422A8D">
          <dt>kmipCertificateFault (Client certificate expired)</dt>
          <dd>The client certificate has expired.</dd>
          <dd>Create a new CSR using the <apiname>GetClientCertificateSigningRequest</apiname> API method and have it signed making sure new expiration date is at least 30 days in the future. Use the <apiname>ModifyKeyServerKmip</apiname> API method to replace the expired client certificate with the new certificate.</dd>
        </dlentry>
        <dlentry id="GUID-E480346C-5D72-464E-9B5A-B0DA39B34D3D">
          <dt>kmipCertificateFault (Invalid root certification authority (CA) certificate)</dt>
          <dd>The root CA certificate is invalid.</dd>
          <dd>Make sure that the correct certificate was provided. If needed, reacquire the certificate from the root CA. Use the <apiname>ModifyKeyServerKmip</apiname> API method to install the correct certificate.</dd>
        </dlentry>
        <dlentry id="GUID-D7011E3E-330B-4E55-9668-15ABE3F0BD37">
          <dt>kmipCertificateFault (Invalid client certificate)</dt>
          <dd>The client certificate is invalid.</dd>
          <dd>Make sure that the correct KMIP client certificate is installed. The root CA of the client certificate should be installed on the external key management server. If you need to update the client certificate, use the <apiname>ModifyKeyServerKmip</apiname> API method to do so.</dd>
        </dlentry>
        <dlentry id="GUID-14A540FA-83D6-4AC9-8966-B95E54FCB1D6">
          <dt>kmipServerFault (Connection failure)</dt>
          <dd>One or more of the nodes cannot access the external key management server.</dd>
          <dd>The key server ID is provided in the fault details. Ensure that the server is functional and reachable via the management network. If only some nodes are unable to access the external key management server, the nodes that are unable to reach the key server are listed in the fault details. Perform troubleshooting at the network or specific node level to determine why only some of the nodes can access the external key management server.</dd>
        </dlentry>
        <dlentry id="GUID-8FE2EEAA-CDFC-4B28-A0CC-7E9F506EF215">
          <dt>kmipServerFault (Authentication failure)</dt>
          <dd>One or more of the nodes cannot authenticate with the external key management server.</dd>
          <dd>Ensure that the correct root CA and KMIP client certificates are in use. If you need to update any of the certificates, use the <apiname>ModifyKeyServerKmip</apiname> method to install the correct certificate.</dd>
        </dlentry>
        <dlentry id="GUID-3D043D0D-FEF8-42B7-9056-B294D5035739">
          <dt>kmipServerFault (Server error)</dt>
          <dd>The external key management server has an error.</dd>
          <dd>The error details are provided in the fault details. You might need to troubleshoot the external key management server based on the error.</dd>
        </dlentry>
        <dlentry id="GUID-1490B06D-60E1-47B2-B351-A8A3BD5FA428">
          <dt>memoryEccThreshold</dt>
          <dd>A large number of correctable or uncorrectable ECC errors have been detected. This fault uses the following severities based on urgency:<simpletable frame="all" relcolwidth="1.56* 1.0* 1.23*"><sthead><stentry>Event</stentry><stentry>Severity</stentry><stentry>Description</stentry></sthead><strow><stentry>A single DIMM cErrorCount reaches cDimmCorrectableErrWarnThreshold.</stentry><stentry>Warning</stentry><stentry>Correctable ECC memory errors above threshold on DIMM: &lt;Processor&gt; &lt;DIMM Slot&gt;</stentry></strow><strow><stentry>A single DIMM cErrorCount stays above cDimmCorrectableErrWarnThreshold until cErrorFaultTimer expires for the DIMM.</stentry><stentry>Error</stentry><stentry>Correctable ECC memory errors above threshold on DIMM: &lt;Processor&gt; &lt;DIMM&gt;</stentry></strow><strow><stentry>A memory controller reports cErrorCount above cMemCtlrCorrectableErrWarnThreshold, and cMemCtlrCorrectableErrWarnDuration is specified.</stentry><stentry>Warning</stentry><stentry>Correctable ECC memory errors above threshold on memory controller: &lt;Processor&gt; &lt;Memory Controller&gt;</stentry></strow><strow><stentry>A memory controller reports cErrorCount above cMemCtlrCorrectableErrWarnThreshold until cErrorFaultTimer expires for the memory controller.</stentry><stentry>Error</stentry><stentry>Correctable ECC memory errors above threshold on DIMM: &lt;Processor&gt; &lt;DIMM&gt;</stentry></strow><strow><stentry>A single DIMM reports a uErrorCount above zero, but less than cDimmUncorrectableErrFaultThreshold.</stentry><stentry>Warning</stentry><stentry>Uncorrectable ECC memory error(s) detected on DIMM: &lt;Processor&gt; &lt;DIMM Slot&gt;</stentry></strow><strow><stentry>A single DIMM reports a uErrorCount of at least cDimmUncorrectableErrFaultThreshold.</stentry><stentry>Error</stentry><stentry>Uncorrectable ECC memory error(s) detected on DIMM: &lt;Processor&gt; &lt;DIMM Slot&gt;</stentry></strow><strow><stentry>A memory controller reports a uErrorCount above zero, but less than cMemCtlrUncorrectableErrFaultThreshold.</stentry><stentry>Warning</stentry><stentry>Uncorrectable ECC memory error(s) detected on memory controller: &lt;Processor&gt; &lt;Memory Controller&gt;</stentry></strow><strow><stentry>A memory controller reports a uErrorCount of at least cMemCtlrUncorrectableErrFaultThreshold.</stentry><stentry>Error</stentry><stentry>Uncorrectable ECC memory error(s) detected on memory controller: &lt;Processor&gt; &lt;Memory Controller&gt;</stentry></strow></simpletable></dd>
          <dd>To resolve this fault, contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-4C05DE00-5649-4649-B2E7-F10FE82AA54A">
          <dt>memoryUsageThreshold</dt>
          <dd>Memory usage is above normal. This fault uses the following severities based on urgency:<note id="GUID-C724BA52-821C-4416-ABFC-61D0CA423D85">See the <b>Details</b> heading for the error fault for more detailed information on the type of fault.</note><simpletable frame="all" relcolwidth="1.0* 1.0*"><sthead><stentry>Severity</stentry><stentry>Description</stentry></sthead><strow><stentry>Warning</stentry><stentry>System memory is low.</stentry></strow><strow><stentry>Error</stentry><stentry>System memory is very low. </stentry></strow><strow><stentry>Critical</stentry><stentry>System memory is completely consumed.</stentry></strow></simpletable></dd>
          <dd>To resolve this fault, contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-E86CA549-BB69-4A77-B460-5A7A38C532B0">
          <dt>metadataClusterFull</dt>
          <dd>There is not enough free metadata storage space to support a single node loss. See the <apiname>GetClusterFullThreshold</apiname> API method for details on cluster fullness levels. This cluster fault indicates one of the following conditions:<ul id="GUID-FB77A830-108D-4F37-ABF4-A9FD4F70C435"><li>stage3Low (Warning): User-defined threshold was crossed. Adjust Cluster Full settings or add more nodes.</li><li>stage4Critical (Error): There is not enough space to recover from a 1-node failure. Creation of volumes, snapshots, and clones is not allowed.</li><li>stage5CompletelyConsumed (Critical)1; No writes or new iSCSI connections are allowed. Current iSCSI connections will be maintained. Writes will fail until more capacity is added to the cluster. Purge or delete data or add more nodes.</li></ul></dd>
          <dd>See <cite>Understanding cluster fullness levels</cite> for more information.</dd>
          <dd>To resolve this fault, purge or delete volumes or add another storage node to the storage cluster. </dd>
        </dlentry>
        <dlentry id="GUID-AAFC2B4D-B182-4CC4-AFD7-FABD9EC10684">
          <dt>mtuCheckFailure</dt>
          <dd>A network device is not configured for the proper MTU size.</dd>
          <dd>To resolve this fault, ensure that all network interfaces and switch ports are configured for jumbo frames (MTUs up to 9000 bytes in size).</dd>
        </dlentry>
        <dlentry id="GUID-303367E1-9074-4FC5-A0F5-4BDCEF090894">
          <dt>networkConfig</dt>
          <dd>This cluster fault indicates one of the following conditions:<ul id="UL_50995B48730648FE96B338B6A2F481C0"><li id="LI_F429D8E75EE04E5FB48770EDA14658E7">An expected network interface is not present.</li><li id="LI_B51A841E776F4251BAC700B6F57DD3E0">A duplicate network interface is present.</li><li id="LI_7366144CC9F14A2CA51F15D2D18B4D7A">A network interface is configured but down.</li><li id="LI_5512872E62BB42108A05B0CB538E12CF">A network interface restart is needed.</li></ul></dd>
          <dd>Contact NetApp Support for assistance. </dd>
        </dlentry>
        <dlentry id="GUID-3010DBD7-2164-4C5B-9547-112ED69FE2E0">
          <dt>noAvailableVirtualNetworkIPAddresses</dt>
          <dd>There are no available virtual network addresses in the block of IP addresses. <ul id="GUID-E6199E83-2548-4144-987D-17B315E72A46"><li>virtualNetworkID # TAG(###) has no available storage IP addresses. Additional nodes cannot be added to the cluster.</li></ul></dd>
          <dd>To resolve this fault, add more IP addresses to the block of virtual network addresses.</dd>
        </dlentry>
        <dlentry id="GUID-A94B67F9-1FD8-47D5-8539-F21733ABEC02">
          <dt>nodeHardwareFault (Network interface &lt;name&gt; is down or cable is unplugged)</dt>
          <dd>A network interface is either down or the cable is unplugged.</dd>
          <dd>To resolve this fault, check network connectivity for the node or nodes.</dd>
        </dlentry>
        <dlentry id="GUID-AFCF461F-3E0C-4CBC-A523-03F3BFC95DDD">
          <dt>nodeHardwareFault (Drive encryption capable state mismatches node's encryption capable state for the drive in slot &lt;node slot&gt;&lt;drive slot&gt;)</dt>
          <dd>A drive does not match encryption capabilities with the storage node it is installed in.</dd>
        </dlentry>
        <dlentry id="GUID-697184A0-6AF2-4458-A1B4-59C044A43269">
          <dt>nodeHardwareFault (Incorrect &lt;drive type&gt; drive size &lt;actual size&gt; for the drive in slot &lt;node slot&gt;&lt;drive slot&gt; for this node type - expected &lt;expected size&gt;)</dt>
          <dd>A storage node contains a drive that is the incorrect size for this node. </dd>
        </dlentry>
        <dlentry id="GUID-A1811F12-D773-42DA-9E66-5A89D8349575">
          <dt>nodeHardwareFault (Unsupported drive detected in slot &lt;node slot&gt;&lt;drive slot&gt;; drive statistics and health information will be unavailable)</dt>
          <dd>A storage node contains a drive it does not support.</dd>
        </dlentry>
        <dlentry id="GUID-632A6EBE-7047-445C-B36B-F425B2D2F8A6">
          <dt>nodeHardwareFault (The drive in slot &lt;node slot&gt;&lt;drive slot&gt; should be using firmware version &lt;expected version&gt;, but is using unsupported version &lt;actual version&gt;)</dt>
          <dd>A storage node contains a drive running an unsupported firmware version.</dd>
        </dlentry>
        <dlentry id="GUID-5AEC3ACF-6A6C-438C-A1D9-EECC2CB7ED16">
          <dt>nodeMaintenanceMode</dt>
          <dd>A node has been placed in maintenance mode. This fault uses the following severities based on urgency:<simpletable frame="all" relcolwidth="1.0* 1.0*"><sthead><stentry>Severity</stentry><stentry>Description</stentry></sthead><strow><stentry>Warning</stentry><stentry>Indicates that the node is still in maintenance mode.</stentry></strow><strow><stentry>Error</stentry><stentry>Indicates that maintenance mode has failed to disable, most likely due to failed or active standbys.</stentry></strow></simpletable></dd>
          <dd>To resolve this fault, disable maintenance mode once maintenance completes. If the Error level fault persists, contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-0E8D5957-824F-4EB5-BEAE-5695811EBAEC">
          <dt>nodeOffline</dt>
          <dd>Element software cannot communicate with the specified node.</dd>
          <dd>To resolve this fault, check network connectivity  and network cabling of the cluster. If there are no network problems, contact NetApp Support for a node replacement.</dd>
        </dlentry>
        <dlentry id="GUID-FC29DC4D-5FDA-4866-BC47-F5481BDD2AB9">
          <dt>notUsingLACPBondMode</dt>
          <dd>LACP bonding mode is not configured. </dd>
          <dd>To resolve this fault, use LACP bonding when deploying storage nodes; clients might experience performance issues if LACP is not enabled and properly configured.</dd>
        </dlentry>
        <dlentry id="GUID-3918C0FE-CDA4-4D47-83E6-1098C3368808">
          <dt>ntpServerUnreachable</dt>
          <dd>The storage cluster cannot communicate with the specified NTP server or servers. </dd>
          <dd>To resolve this fault, check the NTP server configuration, network, and firewall.</dd>
        </dlentry>
        <dlentry id="GUID-67A9CB8F-D7C8-4A73-BB99-3BD1EBDE321D">
          <dt>ntpTimeNotInSync</dt>
          <dd>The difference between storage cluster time and the specified NTP server time is too large. The storage cluster cannot correct the difference automatically.</dd>
          <dd>To resolve this fault, use NTP servers that are internal to your network, rather than the installation defaults. If you are using internal NTP servers and the issue persists, contact NetApp Support for assistance. </dd>
        </dlentry>
        <dlentry id="GUID-EF9D16D6-88B2-452B-8FF5-DB7F0527410B">
          <dt>nvramDeviceStatus</dt>
          <dd>An NVRAM device has an error, is failing, or has failed. This fault has the following severities:<simpletable frame="all" relcolwidth="1.0* 1.0*"><sthead><stentry>Severity</stentry><stentry>Description</stentry></sthead><strow><stentry>Warning</stentry><stentry>A warning has been detected by the hardware. This condition may be transitory, such as a temperature warning. <ul id="GUID-7498E214-683E-424A-A92E-04F41955C9FE"><li>nvmLifetimeError</li><li>nvmLifetimeStatus</li><li>energySourceLifetimeStatus</li><li>energySourceTemperatureStatus</li><li>warningThresholdExceeded</li></ul></stentry></strow><strow><stentry>Error</stentry><stentry>An Error or Critical status has been detected by the hardware. The cluster master attempts to remove the slice drive from operation (this generates a drive removal event). If secondary slice services are not available the drive will not be removed. Errors returned in addition to the Warning level errors:<ul id="GUID-CA83E1E5-4591-4195-BA12-EBE20B0CE5CC"><li>NVRAM device mount point doesn't exist.</li><li>NVRAM device partition doesn't exist.</li><li>NVRAM device partition exists, but not mounted.</li></ul></stentry></strow><strow><stentry>Critical</stentry><stentry>An Error or Critical status has been detected by the hardware. The cluster master attempts to remove the slice drive from operation (this generates a drive removal event). If secondary slice services are not available the drive will not be removed.<ul id="GUID-AA9A39E6-EF55-4FCD-84F1-6817BDE198E4"><li>persistenceLost</li><li>armStatusSaveNArmed</li><li>csaveStatusError</li></ul></stentry></strow></simpletable></dd>
          <dd>Replace any failed hardware in the node. If this does not resolve the issue, contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-27135ED2-94B1-4D38-B3C0-25E3D947EB40">
          <dt>powerSupplyError</dt>
          <dd>This cluster fault indicates one of the following conditions:<ul id="UL_1938E1CF841C4FEF83DCD8513294E912"><li id="LI_383EB3DB1FD2412197AA8A12B23ECCBC">A power supply is not present.</li><li id="LI_7EE6913EFD874A419460DEC1B2074FA3">A power supply has failed.</li><li id="LI_A05D972088CB4C26A4DEAFBEE5141FDA">A power supply has no input or the input is out of range.</li></ul></dd>
          <dd>To resolve this fault, verify that redundant power is supplied to all nodes. Contact NetApp Support if the issue persists. </dd>
        </dlentry>
        <dlentry id="GUID-79C7EF2E-1571-4C1A-BB8C-718DFC6C4C91">
          <dt>provisionedSpaceTooFull</dt>
          <dd>The overall provisioned capacity of the storage cluster is too full. </dd>
          <dd>To resolve this fault, add more provisioned space, or delete and purge volumes or snapshots.</dd>
        </dlentry>
        <dlentry id="GUID-67521DE9-B01B-4986-9AA1-16085AB9A338">
          <dt>remoteRepAsyncDelayExceeded</dt>
          <dd>The configured asynchronous delay for replication has been exceeded.</dd>
        </dlentry>
        <dlentry id="GUID-EBD80821-8961-4D1F-BAD5-44D2952ACE04">
          <dt>remoteRepClusterFull</dt>
          <dd>The volumes have paused remote replication because the target storage cluster is too full. </dd>
          <dd>To resolve this fault, free up some space on the target storage cluster.</dd>
        </dlentry>
        <dlentry id="GUID-B75B33AA-7991-4D6D-8DE9-859E7AA4ED96">
          <dt>remoteRepSnapshotClusterFull</dt>
          <dd>The volumes have paused remote replication of snapshots because the target storage cluster is too full. </dd>
          <dd>To resolve this fault, free up some space on the target storage cluster.</dd>
        </dlentry>
        <dlentry id="GUID-F59DE3EA-4C36-436C-AB19-ABC80763F913">
          <dt>remoteRepSnapshotsExceededLimit</dt>
          <dd>The volumes have paused remote replication of snapshots because the target storage cluster volume has exceeded its snapshot limit. </dd>
          <dd>To resolve this fault, remove some snapshots on the remote cluster.</dd>
        </dlentry>
        <dlentry id="GUID-73FEC383-DCD1-4404-86DA-E14A3F1DF80A">
          <dt>scheduleActionError</dt>
          <dd>One or more of the scheduled activities ran, but failed. </dd>
          <dd>The fault clears if the scheduled activity runs again and succeeds, if the scheduled activity is deleted, or if the activity is paused and resumed.</dd>
        </dlentry>
        <dlentry id="GUID-CE8784A7-5CC3-41E3-BEB8-AC4AA1FCECB9">
          <dt>sensorReadingFailed</dt>
          <dd>The Baseboard Management Controller (BMC) self-test failed or a sensor could not communicate with the BMC.</dd>
          <dd>Contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-95B24CD0-C84B-49E5-959A-5B6EC6C8F90A">
          <dt>serviceNotRunning</dt>
          <dd>A required service is  not running.</dd>
          <dd>Contact NetApp Support for assistance. </dd>
        </dlentry>
        <dlentry id="GUID-6FB65D2E-8886-4509-85D8-C091851AC92D">
          <dt>sliceServiceTooFull</dt>
          <dd>A slice service has too little provisioned capacity assigned to it. </dd>
          <dd>To resolve this fault, add more storage nodes or contact NetApp Support.</dd>
        </dlentry>
        <dlentry id="GUID-18AFBE02-F8E4-4F2D-AE1A-1C4A88C0A92D">
          <dt>sliceServiceUnhealthy</dt>
          <dd>The system has detected that a slice service is unhealthy and is automatically decommissioning it. <ul id="GUID-142587E5-C12A-4B84-9B1B-02CA34CBD104"><li>Severity = Warning: No action is taken. This warning period will expire in 6 minutes.</li><li>Severity = Error: The system is automatically decommissioning data and re-replicating its data to other healthy drives.</li></ul>Check for network connectivity issues and hardware errors. There will be other faults if specific hardware components have failed. The fault will clear when the slice service is accessible or when the service has been decommissioned.</dd>
        </dlentry>
        <dlentry id="GUID-D9962F95-A171-462C-9A4D-E57C8C52875E">
          <dt>sshEnabled</dt>
          <dd>The SSH service is enabled on one  or more nodes in the storage cluster.</dd>
          <dd>To resolve this fault, disable the SSH service on the node or nodes.</dd>
        </dlentry>
        <dlentry id="GUID-C07F3D1F-0D49-43EB-B48D-E8BE11033860">
          <dt>sslCertificateExpiration</dt>
          <dd>The SSL certificate associated with this node is nearing expiration or has expired. This fault uses the following severities based on urgency: <simpletable frame="all" relcolwidth="1.0* 1.0*"><sthead><stentry>Severity</stentry><stentry>Description</stentry></sthead><strow><stentry>Warning</stentry><stentry>Certificate expires within 30 days.</stentry></strow><strow><stentry>Error</stentry><stentry>Certificate expires within 7 days.</stentry></strow><strow><stentry>Critical</stentry><stentry>Certificate expires within 3 days or has already expired.</stentry></strow></simpletable></dd>
          <dd>To resolve this fault, renew the SSL certificate. If needed, contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-D585E059-27D8-4BAD-918D-0D2F605FEB57">
          <dt>strandedCapacity</dt>
          <dd>A single node accounts for more than half of the storage cluster capacity. </dd>
          <dd>In order to maintain data redundancy, the system reduces the capacity of the largest node so that some of its block capacity is stranded (not used). To resolve this fault, add more drives to existing storage nodes or add storage nodes to the cluster.</dd>
        </dlentry>
        <dlentry id="GUID-BB497B03-87BB-472D-BD6C-D3D29756773E">
          <dt>tempSensor</dt>
          <dd>A temperature sensor is reporting higher than normal temperatures. This fault can be triggered in conjunction with powerSupplyError or fanSensor faults.</dd>
          <dd>To resolve this fault, check for airflow obstructions near the storage cluster. If needed, contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-7D372D8C-93E4-4C38-95B4-36E120993B94">
          <dt>upgrade</dt>
          <dd>An upgrade has been in progress for more than 24 hours.</dd>
          <dd>To resolve this fault, resume the upgrade or contact NetApp Support for assistance. </dd>
        </dlentry>
        <dlentry id="GUID-B3C3F292-5CFE-4847-891C-9E8858480C02">
          <dt>unbalancedMixedNodes</dt>
          <dd>A single node accounts for more than one-third of the storage cluster's capacity. </dd>
          <dd>Contact NetApp Support for assistance. </dd>
        </dlentry>
        <dlentry id="GUID-BD279698-0BEF-446D-B0C2-3C4E6AF05F5C">
          <dt>unresponsiveService</dt>
          <dd>A system service has become unresponsive.</dd>
          <dd>Contact NetApp Support for assistance. </dd>
        </dlentry>
        <dlentry id="GUID-28D54CD2-2A65-48C3-BDDA-564502575430">
          <dt>virtualNetworkConfig</dt>
          <dd>This cluster fault indicates one of the following conditions:<ul id="UL_94A14B180E9D42B19416C8AB2BBB7678"><li id="LI_A4E7FD2124C94CAEB0885CEE0753CECF">An interface is not present.</li><li id="LI_7350B589864243DDB01C2A8ABFF48DE6">There is an incorrect namespace on an interface.</li><li id="LI_8AAC16FCA3DF4DD686A500D9E73395CC">	There is an incorrect network mask.</li><li id="LI_BF154DCCDC4A4CF5839FD91F47958F50">There is an incorrect IP address.</li><li id="LI_50C73CBBA650484A844015A42310FBDF">An interface is not up and running.</li><li id="LI_17635A40166C46819BFFFABAD1ED5C35">There is a superfluous interface on a node.</li></ul></dd>
          <dd>Contact NetApp Support for assistance. </dd>
        </dlentry>
        <dlentry id="GUID-1D803412-7020-49F3-A5F1-72FF760D1BAA">
          <dt>volumesDegraded</dt>
          <dd>Secondary volumes have not yet completely replicated and synchronized. </dd>
          <dd>This fault  is cleared when the synchronisation is complete.</dd>
          <dd>If the fault persists, check for network connectivity issues and hardware errors.</dd>
        </dlentry>
        <dlentry id="GUID-38174E3E-ADD6-4633-A0F4-9FA188A01CAB">
          <dt>volumesOffline</dt>
          <dd>One or more volumes in the storage cluster are offline.</dd>
          <dd>Contact NetApp Support for assistance. </dd>
        </dlentry>
      </dl>
    </section>
  </refbody>
</reference>