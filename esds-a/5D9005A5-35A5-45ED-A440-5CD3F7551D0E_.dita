<!DOCTYPE task PUBLIC "-//OASIS//DTD DITA Task//EN" "file:///C:/InfoShare/Web/Author/ASP/DocTypes/dita-oasis/1.2/technicalContent/dtd/task.dtd">
<task id="GUID-5D9005A5-35A5-45ED-A440-5CD3F7551D0E" xml:lang="en" xmlns:ditaarch="http://dita.oasis-open.org/architecture/2005/">
  <title id="GUID-328993EE-E48C-4A5F-8D16-8BD49512BC59">Upgrade <ph conref="ISH_variables.dita#variables/SDS-SHORT">SolidFire eSDS</ph> clusters</title>
  <shortdesc>You can use Ansible to perform non-disruptive rolling upgrades on your <ph conref="ISH_variables.dita#variables/SDS-SHORT">SolidFire eSDS</ph> cluster. Using the <uicontrol>nar_solidfire_sds_upgrade</uicontrol> role provided by NetApp, Ansible performs rolling upgrades one node at a time while maintaining data availability to all the volumes.</shortdesc>
  <prolog>
    <metadata>
      <keywords>
        <keyword>solidfire esds</keyword>
        <keyword>upgrading solidfire esds clusters</keyword>
        <keyword>ansible role</keyword>
      </keywords>
    </metadata>
  </prolog>
  <taskbody>
    <prereq id="GUID-CD88BA04-53B3-4C87-AA2B-55AC5E69A848">Ensure that the following conditions are met before you upgrade:<ul id="GUID-84F570FB-B147-4674-9E19-259DE3E35171"><li>There are no cluster faults in the Element UI.</li><li>The inventory file is up to date with the current RPM file build information and details about cluster member nodes.</li><li>The hosts are defined in the inventory file by using IP addresses (and not fully qualified domain names [FQDNs]).<note type="attention" id="GUID-E0E57D41-04D8-40C9-9712-239EF738A51A">The upgrade will fail if you define the hosts by using FQDNs.</note></li><li>The hosts are defined in the inventory file using the format in the following example:<screen xml:space="preserve">hosts:
  10.117.136.26:
  10.117.136.27:</screen></li><li>The number of nodes in your inventory file is the same as the number of nodes in the cluster that you are upgrading. If there is a number mismatch, the upgrade procedure will fail with a error similar to the following example: <codeph>"Cluster 10.194.79.151 consists of more nodes than what has been specified for upgrade!"</codeph></li><li>The inventory file has the following variables specified: <varname>sf_mgmt_virt_ip </varname> (MVIP), <varname>sf_cluster_admin_username</varname>, <varname>sf_cluster_admin_passwd</varname>, and <varname>solidfire_element_rpm</varname> (path to the new RPM file).</li></ul></prereq>
    <context id="GUID-7EBD6C27-4578-4B2F-9567-D059503DB7FC">Here is an overview of what happens in the background during the upgrade process after you run the playbook:<ul id="GUID-14C10D3F-12CA-4D92-A0E6-EFE51A4393B5"><li>The information you entered in the inventory file is validated.</li><li>Node information is collected.</li><li>RPM is installed on all nodes included in the inventory file in parallel.</li><li>After the RPM is installed on each node, each <ph conref="ISH_variables.dita#variables/SDS-SHORT">SolidFire eSDS</ph> node is upgraded one at a time. Each node is automatically placed in maintenance mode. You do not have to manually enable maintenance mode if you are running the upgrade playbook. </li><li>After the first node is placed in maintenance mode, volumes hosted on that <ph conref="ISH_variables.dita#variables/SDS-SHORT">SolidFire eSDS</ph> node are failed over to remaining <ph conref="ISH_variables.dita#variables/SDS-SHORT">SolidFire eSDS</ph> nodes in the cluster.</li><li>SolidFire service is restarted to pick up the latest version of the application.</li><li>Maintenance mode is deactivated for the node, and the cluster waits for the node to recover.</li><li>After the node comes back online, the cluster is balanced.</li><li>The same process is repeated for all the nodes in the cluster.</li><li>After all the nodes are upgraded, the cluster shows the latest version.</li></ul><note id="GUID-9CDDE737-4AB5-4265-935A-043CBF33F0AD">If an error happens during the upgrade or your cluster experiences a fault, the upgrade does not stop. It progresses to the extent that it can and prints out a list of all the nodes that were successfully and unsuccessfully upgraded. After you fix any errors, you can rerun the playbook or reject the file to complete the upgrade process.</note><note type="attention" id="GUID-6667022E-E463-449E-ABD9-23441FDBCD87">If the upgrade fails because of a fault, you must resolve it and resume the upgrade. The cluster will remain in upgrade status until the upgrade is complete. If the fault is not cleared by Element while the cluster is in upgrade status, you must contact NetApp Support. Depending on the nature of the fault and if it is safe to do so, Support might instruct you to add the <varname>yes_i_want_to_ignore_cluster_faults</varname> variable and set to <varname>true</varname> in your upgrade playbook and re-run playbook. Do not attempt this without consulting with Support.</note></context>
    <steps id="GUID-E838F850-E35F-49BA-BE3F-30FA28E19B94">
      <step>
        <cmd>Run the <cmdname>ansible-galaxy</cmdname> command to install all the roles relevant to <ph conref="ISH_variables.dita#variables/SDS-SHORT">SolidFire eSDS</ph> provided by NetApp. </cmd>
        <info>The upgrade role is <uicontrol>nar_solidfire_sds_upgrade</uicontrol> and will be installed when you run the following command: <screen xml:space="preserve">$  ansible-galaxy install git+https://github.com/netapp/ansible.git
- extracting ansible to ~/.ansible/roles/ansible
- ansible was installed successfully
</screen>You can also manually install the role by copying it from the NetApp GitHub repository and placing the role in the <filepath>~/.ansible/roles</filepath> directory. NetApp provides a README file, which includes information about how to run a role.<note id="GUID-98B818D1-30BB-495B-95BF-42DB2DDBF662">Ensure that you always download the latest versions of the roles. </note></info>
      </step>
      <step>
        <cmd>Move the roles that you downloaded up one directory from where they were installed.</cmd>
        <info>
          <screen xml:space="preserve">$ mv ~/.ansible/roles/ansible/nar_solidfire_sds_* ~/.ansible/roles/</screen>
        </info>
      </step>
      <step>
        <cmd>Run the <cmdname>ansible-galaxy role list</cmdname> command to ensure that Ansible is configured to utilize the new roles.</cmd>
        <info>
          <screen xml:space="preserve">$ ansible-galaxy role list
# ~/.ansible/roles
- nar_solidfire_sds_install, (unknown version)
- nar_solidfire_sds_upgrade, (unknown version)
- ansible, (unknown version)
- nar_solidfire_sds_compliance, (unknown version)
</screen>
        </info>
      </step>
      <step>
        <cmd>Create the playbook to use for upgrades. If you already have a playbook and want to use that, ensure that you specify the <uicontrol>nar_solidfire_sds_upgrade</uicontrol> role in this playbook.</cmd>
      </step>
      <step>
        <cmd>Run the playbook:</cmd>
        <info>
          <screen xml:space="preserve">$ ansible-playbook -i inventory.yaml playbook_upgrade_sample.yaml</screen>The playbook name used here is an example. You must replace it with the name of your playbook. Running the playbook validates the information that you entered in the inventory file and installs the RPM on all the nodes listed in the inventory. You can check the Ansible output to verify that each node is upgraded. </info>
      </step>
      <step>
        <cmd>After the upgrade is complete, verify each node to ensure that the new version is running by using the Element UI or the cluster API. </cmd>
      </step>
    </steps>
  </taskbody>
</task>