<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "file:///C:/InfoShare/Web/Author/ASP/DocTypes/dita-oasis/1.2/technicalContent/dtd/reference.dtd">
<reference id="GUID-5C1C462C-1939-45E3-8684-5316E08D54A6" xml:lang="en" xmlns:ditaarch="http://dita.oasis-open.org/architecture/2005/">
  <title id="GUID-E133F6B9-3ABF-48F7-AD12-021C570B744F">HCI_Element-system_messaging_conrefs</title>
  <shortdesc>Contains conrefs we've converted from old content as well as new items.</shortdesc>
  <prolog>
    <metadata>
      <keywords />
    </metadata>
  </prolog>
  <refbody>
    <section id="SECTION_6B7423FA01A744CFA23D5F0DF4C06CEF">
      <dl>
        <dlentry id="GUID-D9A2FB33-FA3F-47E6-AB8F-83AA97728DB5">
          <dt>authenticationServiceFault</dt>
          <dd>The Authentication Service on one or more cluster nodes is not functioning as expected.</dd>
          <dd>Contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-8E9C2395-A604-4360-95D1-CD6506145F83">
          <dt>availableVirtualNetworkIPAddressesLow</dt>
          <dd>The number of virtual network addresses in the block of IP addresses is low.</dd>
          <dd> To resolve this fault, add more IP addresses to the block of virtual network addresses.</dd>
        </dlentry>
        <dlentry id="GUID-CDE93C32-14B3-4A12-B5FC-10C49B28ACE2">
          <dt>blockClusterFull</dt>
          <dd>There is not enough free block storage space to support a single node loss. See the <apiname>GetClusterFullThreshold</apiname> API method for details on cluster fullness levels. This cluster fault indicates one of the following conditions:<ul id="GUID-6D17ECAB-6075-4634-939E-8001530FEE84"><li>stage3Low (Warning): User-defined threshold was crossed. Adjust Cluster Full settings or add more nodes.</li><li>stage4Critical (Error): There is not enough space to recover from a 1-node failure. Creation of volumes, snapshots, and clones is not allowed.</li><li>stage5CompletelyConsumed (Critical)1; No writes or new iSCSI connections are allowed. Current iSCSI connections will be maintained. Writes will fail until more capacity is added to the cluster.</li></ul></dd>
          <dd>To resolve this fault, purge or delete volumes or add another storage node to the storage cluster. </dd>
        </dlentry>
        <dlentry id="GUID-3D1109AA-6371-4BCB-96DA-B715B2C38B0E">
          <dt>blocksDegraded</dt>
          <dd>Block data is no longer fully replicated due to a failure. </dd>
          <dd>
            <simpletable frame="all" relcolwidth="1.0* 1.0*">
              <sthead>
                <stentry>Severity</stentry>
                <stentry>Description</stentry>
              </sthead>
              <strow>
                <stentry>Warning</stentry>
                <stentry>Only two complete copies of the block data are accessible.</stentry>
              </strow>
              <strow>
                <stentry>Error</stentry>
                <stentry>Only a single complete copy of the block data is accessible.</stentry>
              </strow>
              <strow>
                <stentry>Critical</stentry>
                <stentry>No complete copies of the block data are accessible.</stentry>
              </strow>
            </simpletable>
            <note id="GUID-EE1EEB6B-4019-42B4-8CBB-FE5101078B37">The warning status can only occur on a Triple Helix system.</note>
          </dd>
          <dd>To resolve this fault, restore any offline nodes or block services, or contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-A2877CC0-3FB4-4441-8139-27C7588107D7">
          <dt>blockServiceTooFull</dt>
          <dd>A block service is using too much space.</dd>
          <dd>To resolve this fault, add more provisioned capacity.</dd>
        </dlentry>
        <dlentry id="GUID-4ECE12F9-0216-4C4C-ACB6-7006C1181847">
          <dt>blockServiceUnhealthy</dt>
          <dd>A block service has been detected as unhealthy: <ul id="GUID-0A2D245B-F708-4086-9043-CC9F95B61706"><li>Severity = Warning: No action is taken. This warning period will expire in cTimeUntilBSIsKilledMSec=330000 milliseconds.</li><li>Severity = Error: The system is automatically decommissioning data and re-replicating its data to other healthy drives.</li><li>Severity = Critical: There are failed block services on several nodes greater than or equal to the replication count (2 for double helix). Data is unavailable and bin syncing will not finish.</li></ul>Check for network connectivity issues and hardware errors. There will be other faults if specific hardware components have failed. The fault will clear when the block service is accessible or when the service has been decommissioned.</dd>
        </dlentry>
        <dlentry id="GUID-98CDD472-B1B3-4492-979D-AFDEB704ABA4">
          <dt>clockSkewExceedsFaultThreshold</dt>
          <dd>Time skew between the Cluster master and the node which is presenting a token exceeds the recommended threshold. Storage cluster cannot correct the time skew between the nodes automatically.</dd>
          <dd>To resolve this fault, use NTP servers that are internal to your network, rather than the installation defaults. If you are using an internal NTP server, contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-FB1758D5-BFD6-46D6-AEC4-5C075BEF34F1">
          <dt>clusterCannotSync</dt>
          <dd>There is an out-of-space condition and data on the offline block storage drives cannot be synced to drives that are still active.</dd>
          <dd>To resolve this fault, add more storage.</dd>
        </dlentry>
        <dlentry id="GUID-F85E36F8-5557-4BCB-B1B8-E6A9A850D1EF">
          <dt>clusterFull</dt>
          <dd>There is no more free storage space in the storage cluster.</dd>
          <dd> To resolve this fault, add more storage.</dd>
        </dlentry>
        <dlentry id="GUID-0D207AC8-1241-4CBB-A220-AA95DE264B5A">
          <dt>clusterIOPSAreOverProvisioned</dt>
          <dd>Cluster IOPS are over provisioned. The sum of all minimum QoS IOPS is greater than the expected IOPS of the cluster. Minimum QoS cannot be maintained for all volumes simultaneously.</dd>
          <dd>To resolve this issue, lower the minimum QoS IOPS settings for volumes.</dd>
        </dlentry>
        <dlentry id="GUID-E4DEF3CB-9730-49A4-91A8-5FBBBAB44333">
          <dt>disableDriveSecurityFailed</dt>
          <dd>The cluster is not configured to enable drive security (Encryption at Rest), but at least one drive has drive security enabled, meaning that disabling drive security on those drives failed. This fault is logged with “Warning” severity.</dd>
          <dd>To resolve this fault, check the fault details for the reason why drive security could not be disabled. Possible reasons are:<ul id="GUID-D531C0CC-F2B8-4D7E-A498-BEE84D3B119B"><li>The encryption key could not be acquired, investigate the problem with access to the key or the external key server.</li><li>The disable operation failed on the drive, determine whether the wrong key could possibly have been acquired.</li></ul>If neither of these are the reason for the fault, the drive might need to be replaced. <p>You can attempt to recover a drive that does not successfully disable security even when the correct authentication key is provided. To perform this operation, remove the drive(s) from the system by moving it to Available, perform a secure erase on the drive and move it back to Active.</p></dd>
        </dlentry>
        <dlentry id="GUID-E61A4A6C-C4A9-455D-909C-D7479554A268">
          <dt>disconnectedClusterPair</dt>
          <dd>A cluster pair is disconnected or configured incorrectly. Check network connectivity between the clusters.</dd>
        </dlentry>
        <dlentry id="GUID-BFE6244F-E3A8-4211-89D8-BF46AAD1D48F">
          <dt>disconnectedRemoteNode</dt>
          <dd>A remote node is either disconnected or configured incorrectly. Check network connectivity between the nodes.</dd>
        </dlentry>
        <dlentry id="GUID-9B3B805D-4285-469D-B2BA-CCDB8E975188">
          <dt>disconnectedSnapMirrorEndpoint</dt>
          <dd>A remote SnapMirror endpoint is disconnected or configured incorrectly. Check network connectivity between the cluster and the remote SnapMirrorEndpoint.</dd>
        </dlentry>
        <dlentry id="GUID-2A648FCF-32B6-4FB4-9FCE-1D08047DA5D4">
          <dt>driveAvailable</dt>
          <dd>One or more drives are available in the cluster. In general, all clusters should have all drives added and none in the available state. If this fault appears unexpectedly, contact NetApp Support. </dd>
          <dd>To resolve this fault, add any available drives to the storage cluster.</dd>
        </dlentry>
        <dlentry id="GUID-81705FFC-8269-42AF-9C5A-29773673E206">
          <dt>driveFailed</dt>
          <dd>The cluster returns this fault when one or more drives have failed, indicating one of the following conditions:<ul id="GUID-5447F1BB-917D-49DF-B498-3CEF37E04302"><li>The drive manager cannot access the drive.</li><li>The slice or block service has failed too many times, presumably because of drive read or write failures, and cannot restart.</li><li>The drive is missing.</li><li>The master service for the node is inaccessible (all drives in the node are considered missing/failed).</li><li>The drive is locked and the authentication key for the drive cannot be acquired.</li><li>The drive is locked and the unlock operation fails.</li></ul>To resolve this issue:<ul id="GUID-10F0945B-FB4C-4624-8F53-5C1D0C7D0C96"><li>Check network connectivity for the node.</li><li>Replace the drive.</li><li>Ensure that the authentication key is available.</li></ul></dd>
        </dlentry>
        <dlentry id="GUID-EA2DEA4A-49EF-4458-9112-8BFCCA7AB9AB">
          <dt>driveHealthFault</dt>
          <dd>A drive has failed the SMART health check and as a result, the drive’s functions are diminished. There is a Critical severity level for this fault:<ul id="GUID-EAAE3DF8-2113-473E-AAAB-253F6ED95620"><li>Drive with serial: &lt;serial number&gt; in slot: &lt;node slot&gt;&lt;drive slot&gt; has failed the SMART overall health check.</li></ul>To resolve this fault, replace the drive.</dd>
        </dlentry>
        <dlentry id="GUID-0ECB89CE-79F6-4E6D-A14C-BCADAA79ACB1">
          <dt>driveWearFault</dt>
          <dd>A drive's remaining life has dropped below thresholds, but it is still functioning.There are two possible severity levels for this fault: Critical and Warning:<ul id="GUID-F00C65C5-A6A4-4E13-9B2F-43BD11871727"><li>Drive with serial: &lt;serial number&gt; in slot: &lt;node slot&gt;&lt;drive slot&gt; has critical wear levels.</li><li>Drive with serial: &lt;serial number&gt; in slot: &lt;node slot&gt;&lt;drive slot&gt; has low wear reserves.</li></ul>To resolve this fault, replace the drive soon.</dd>
        </dlentry>
        <dlentry id="GUID-410B30F4-929F-423C-9C6D-BF4DF8325E22">
          <dt>duplicateClusterMasterCandidates</dt>
          <dd>
More than one storage cluster master candidate has been detected. Contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-4F4A4940-1493-4755-91C9-D550B85EDEAA">
          <dt>enableDriveSecurityFailed</dt>
          <dd>The cluster is configured to require drive security (Encryption at Rest), but drive security could not be enabled on at least one drive. This fault is logged with “Warning” severity.</dd>
          <dd>To resolve this fault, check the fault details for the reason why drive security could not be enabled. Possible reasons are:<ul id="GUID-A1A82D76-1727-4009-981A-66A9C7592AC6"><li>The encryption key could not be acquired, investigate the problem with access to the key or the external key server.</li><li>The enable operation failed on the drive, determine whether the wrong key could possibly have been acquired. </li></ul>If neither of these are the reason for the fault, the drive might need to be replaced. <p>You can attempt to recover a drive that does not successfully enable security even when the correct authentication key is provided. To perform this operation, remove the drive(s) from the system by moving it to Available, perform a secure erase on the drive and move it back to Active.</p></dd>
        </dlentry>
        <dlentry id="GUID-AB94CD37-B0FB-4C62-80C8-9161E18CF650">
          <dt>ensembleDegraded</dt>
          <dd>Network connectivity or power has been lost to one or more of the ensemble nodes.</dd>
          <dd>To resolve this fault, restore network connectivity or power.</dd>
        </dlentry>
        <dlentry id="GUID-5FFC9191-1817-4302-8B8C-14D81810D8D3">
          <dt>exception</dt>
          <dd>A fault reported that is other than a routine fault. These faults are not automatically cleared from the fault queue. Contact  NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-5C491B24-21D0-445A-A5A9-FF61125D7820">
          <dt>failedSpaceTooFull</dt>
          <dd>A block service is not responding to data write requests. This causes the slice service to run out of space to store failed writes. </dd>
          <dd>To resolve this fault, restore block services functionality to allow writes to continue normally and failed space to be flushed from the slice service.</dd>
        </dlentry>
        <dlentry id="GUID-34A93DA4-488C-4ADF-A2E2-699BEEF2E766">
          <dt>fanSensor</dt>
          <dd>A fan sensor has failed or is missing.</dd>
          <dd>To resolve this fault, replace any failed hardware.</dd>
        </dlentry>
        <dlentry id="GUID-1BBEF429-8A26-4784-A68D-1B2B0D0E0FD9">
          <dt>fibreChannelAccessDegraded</dt>
          <dd>A Fibre Channel node is not responding to other nodes in the storage cluster over its storage IP for a period of time. In this state, the node will then be considered unresponsive and generate a cluster fault. Check network connectivity.</dd>
        </dlentry>
        <dlentry id="GUID-7CE56117-5F51-4DAF-BBF4-2571E3DAA563">
          <dt>fibreChannelAccessUnavailable</dt>
          <dd>All Fibre Channel nodes are unresponsive. The node IDs are displayed. Check network connectivity.</dd>
        </dlentry>
        <dlentry id="GUID-5529D14A-B87B-45A6-8A65-444EE983CF2E">
          <dt>fibreChannelActiveIxL</dt>
          <dd>The IxL Nexus count is approaching the supported limit of 8000 active sessions per Fibre Channel node.<ul id="GUID-5309645D-1EE7-47CD-BA49-471249C36C0F"><li>Best practice limit is 5500.</li><li>Warning limit is 7500.</li><li>Maximum limit (not enforced) is 8192.</li></ul></dd>
          <dd>To resolve this fault, reduce the IxL Nexus count below the best practice limit of 5500.</dd>
        </dlentry>
        <dlentry id="GUID-0914DC5D-85CD-4FC5-B3DA-BBCC13BCE76C">
          <dt>fibreChannelConfig</dt>
          <dd>This cluster fault indicates one of the following conditions:<ul id="UL_0CE768E118864A3A834505562D08FA4B"><li id="LI_6E9EF79A58CE48EEA19AE7113097C2F3">	There is an unexpected Fibre Channel port on a PCI slot.</li><li id="LI_27A79247E67D42D08960258E15940F0C">There is an unexpected Fibre Channel HBA model.</li><li id="LI_DCBD1710293A4F9A9D9C26932FE9BC30">There is a problem with the firmware of a Fibre Channel HBA.</li><li id="LI_6E76ADFC5EF845F2A080C3631F25009A">A Fibre Channel port is not online.</li><li id="LI_478FA4053BEA44D49566C96B505400C7">There is a persistent issue configuring Fibre Channel passthrough.</li></ul>Contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-EE783007-2ED4-4F19-8C5C-C2529405B1E7">
          <dt>fibreChannelIOPS</dt>
          <dd>The total IOPS count is approaching the IOPS limit for Fibre Channel nodes in the cluster. The limits are: <ul id="GUID-9B7DC4AE-7110-4A31-BBA9-17DE8A0734EC"><li>FC0025: 450K IOPS limit at 4K block size per Fibre Channel node.</li><li>FCN001: 625K OPS limit at 4K block size per Fibre Channel node.</li></ul></dd>
          <dd>To resolve this fault, balance the load across all available Fibre Channel nodes.</dd>
        </dlentry>
        <dlentry id="GUID-3A034470-320E-4D4F-8406-9E7A84400BA1">
          <dt>fibreChannelStaticIxL</dt>
          <dd>The IxL Nexus count is approaching the supported limit of 16000 static sessions per Fibre Channel node.<ul id="GUID-A560B710-1850-4B12-A883-1E2C9A27ED43"><li>Best practice limit is 11000.</li><li>Warning limit is 15000.</li><li>Maximum limit (enforced) is 16384.</li></ul></dd>
          <dd>To resolve this fault, reduce the IxL Nexus count below the best practice limit of 11000.</dd>
        </dlentry>
        <dlentry id="GUID-9CF9F755-FBDD-4BCC-9E08-8A6A0161E454">
          <dt>fileSystemCapacityLow</dt>
          <dd>There is insufficient space on one of the filesystems.</dd>
          <dd>To resolve this fault, add more capacity to the filesystem.</dd>
        </dlentry>
        <dlentry id="GUID-0AA54518-E359-4824-A39A-0E1AA95F63C3">
          <dt>fipsDrivesMismatch</dt>
          <dd>
            <p>A non-FIPS drive has been physically inserted into a FIPS capable storage node or a FIPS drive has been physically inserted into a non-FIPS storage node. A single fault is generated per node and lists all drives affected. </p>
            <p>To resolve this fault, remove or replace the mismatched drive or drives in question. </p>
          </dd>
        </dlentry>
        <dlentry id="GUID-F08225CB-FD2D-4204-959A-624CFC8D49C6">
          <dt>fipsDrivesOutOfCompliance</dt>
          <dd>
            <p>The system has detected that Encryption at Rest was disabled after the FIPS Drives feature was enabled. This fault is also generated when the FIPS Drives feature is enabled and a non-FIPS drive or node is present in the storage cluster. </p>
            <p>To resolve this fault, enable Encryption at Rest or remove the non-FIPS hardware from the storage cluster. </p>
          </dd>
        </dlentry>
        <dlentry id="GUID-C9CDF235-0DC9-418D-8184-4CEA66880B1C">
          <dt>fipsSelfTestFailure </dt>
          <dd>The FIPS subsystem has detected a failure during the self test.</dd>
          <dd>Contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-572B05FD-AD5A-44C2-AB50-157703AB249A">
          <dt>hardwareConfigMismatch</dt>
          <dd>This cluster fault indicates one of the following conditions:<ul id="UL_F60E4EA32322468A8210F94444F7C118"><li id="LI_6EC3243BC4AC4B198691A3B668DBC298">The configuration does not match the node definition.</li><li id="LI_D37A35C0D4764EE0B86EAB52B5654939">There is an incorrect drive size for this type of node.</li><li id="LI_CF7948B5150F43F7A0BD44A738608046">An unsupported drive has been detected. A possible reason is that the installed Element version does not recognize this drive. Recommend updating the Element software on this node. </li><li id="LI_DDEB1D7557F346F1B4F35A268D448198">There is a drive firmware mismatch.</li><li id="LI_A3E3104BA98F4BBEACD8B8B7BDE6D6F0">The drive encryption capable state does not match the node.</li></ul>Contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-7F89CAA9-D86B-46DD-9474-A13F45A55E9F">
          <dt>idPCertificateExpiration</dt>
          <dd>The cluster’s service provider SSL certificate for use with a third-party identity provider (IdP) is nearing expiration or has already expired. This fault uses the following severities based on urgency:<simpletable frame="all" relcolwidth="1.0* 1.0*"><sthead><stentry>Severity</stentry><stentry>Description</stentry></sthead><strow><stentry>Warning</stentry><stentry>Certificate expires within 30 days.</stentry></strow><strow><stentry>Error</stentry><stentry>Certificate expires within 7 days.</stentry></strow><strow><stentry>Critical</stentry><stentry>Certificate expires within 3 days or has already expired.</stentry></strow></simpletable></dd>
          <dd>To resolve this fault, update the SSL certificate before it expires. Use the <apiname>UpdateIdpConfiguration</apiname> API method with <codeph>refreshCertificateExpirationTime=true</codeph> to provide the updated SSL certificate.</dd>
        </dlentry>
        <dlentry id="GUID-18DE496B-9948-481F-AEF8-5600F9B0C9F6">
          <dt>inconsistentBondModes</dt>
          <dd>The bond modes on the VLAN device are missing. This fault will display the expected bond mode and the bond mode currently in use.</dd>
        </dlentry>
        <dlentry id="GUID-A504F26D-5F7A-4A3F-B57C-66B45AA46CF5">
          <dt>inconsistentInterfaceConfiguration</dt>
          <dd>The interface configuration is inconsistent.</dd>
          <dd>To resolve this fault, ensure the node interfaces in the storage cluster are consistently configured.</dd>
        </dlentry>
        <dlentry id="GUID-DA50B7EE-EC7D-4D9C-BF39-348336385CFF">
          <dt>inconsistentMtus</dt>
          <dd>This cluster fault indicates one of the following conditions:<ul id="UL_74C4C6730DCC45458D275FD6C904A96D"><li id="LI_302880668869422DBA07FFF5E2765D01">Bond1G mismatch: Inconsistent MTUs have been detected on Bond1G interfaces.</li><li id="LI_83812D2F8AE04D72974523D825EB5DD0">Bond10G mismatch: Inconsistent MTUs have been detected on Bond10G interfaces.</li></ul>This fault displays the node or nodes in question along with the associated MTU value.</dd>
        </dlentry>
        <dlentry id="GUID-DC52CA6F-FE2B-4FDE-867A-71CB5A81AADE">
          <dt>inconsistentRoutingRules</dt>
          <dd>The routing rules for this interface are inconsistent.</dd>
        </dlentry>
        <dlentry id="GUID-A2680BE7-7AB9-4B5B-AB6A-110593F1D1BE">
          <dt>inconsistentSubnetMasks</dt>
          <dd>The network mask on the VLAN device does not match the internally recorded network mask  for the VLAN. This fault displays the expected network mask and the network mask currently in use.</dd>
        </dlentry>
        <dlentry id="GUID-C703DC6D-7A5A-4BC7-BC26-B2D8F20A4AE6">
          <dt>incorrectBondPortCount</dt>
          <dd>The number of bond ports is incorrect.</dd>
        </dlentry>
        <dlentry id="GUID-A77E83E0-4DA9-47F3-9350-A0FB6C2C8059">
          <dt>invalidConfiguredFibreChannelNodeCount</dt>
          <dd>One of the two expected Fibre Channel node connections is degraded. This fault appears when only one Fibre Channel node is connected.</dd>
          <dd>To resolve this fault, check the cluster network connectivity and network cabling, and check for failed services. If there are no network or service problems, contact NetApp Support for a Fibre Channel node replacement. </dd>
        </dlentry>
        <dlentry id="GUID-EB071555-7F69-4066-A076-EED81C9A1C08">
          <dt>irqBalanceFailed</dt>
          <dd>An exception occurred while attempting to balance interrupts.</dd>
          <dd>Contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-789BAE0B-AB8D-4312-94BD-1543882990A1">
          <dt>kmipCertificateFault</dt>
          <dd>
            <ul id="GUID-529617D0-BE16-495F-BA6D-733D40EC9A9C">
              <li>Root Certification Authority (CA) certificate is nearing expiration.<p>To resolve this fault, acquire a new certificate from the root CA with expiration date at least 30 days out and use <cmdname>ModifyKeyServerKmip</cmdname> to provide the updated root CA certificate.</p></li>
            </ul>
            <ul id="GUID-337FEB0A-167A-4336-971E-581F776443A3">
              <li>Client certificate is nearing expiration.<p>To resolve this fault, create a new CSR using <cmdname>GetClientCertificateSigningRequest</cmdname>, have it signed ensuring the new expiration date is at least 30 days out, and use <cmdname>ModifyKeyServerKmip</cmdname> to replace the expiring KMIP client certificate with the new certificate.</p></li>
            </ul>
          </dd>
          <dd>
            <ul id="GUID-C7DA9CD6-2482-4791-A1AA-FB36195545A8">
              <li>Root Certification Authority (CA) certificate has expired.<p>To resolve this fault, acquire a new certificate from the root CA with expiration date at least 30 days out and use <cmdname>ModifyKeyServerKmip</cmdname> to provide the updated root CA certificate.</p></li>
            </ul>
          </dd>
          <dd>
            <ul id="GUID-3860AC2D-0456-47D2-A4D1-146410B88C8A">
              <li>Client certificate has expired.<p>To resolve this fault, create a new CSR using <cmdname>GetClientCertificateSigningRequest</cmdname>, have it signed ensuring the new expiration date is at least 30 days out, and use <cmdname>ModifyKeyServerKmip</cmdname> to replace the expired KMIP client certificate with the new certificate.</p></li>
            </ul>
          </dd>
          <dd>
            <ul id="GUID-5EFBD560-BA09-4BFD-9220-024309D47A08">
              <li>Root Certification Authority (CA) certificate error.<p>To resolve this fault, check that the correct certificate was provided, and, if needed, reacquire the certificate from the root CA. Use <cmdname>ModifyKeyServerKmip</cmdname> to install the correct KMIP client certificate.</p></li>
            </ul>
          </dd>
          <dd>
            <ul id="GUID-8D32CBB3-71D4-4314-AE92-E5C4EF083CAA">
              <li>Client certificate error.<p>To resolve this fault, check that the correct KMIP client certificate is installed. The root CA of the client certificate should be installed on the EKS. Use <cmdname>ModifyKeyServerKmip</cmdname> to install the correct KMIP client certificate.</p></li>
            </ul>
          </dd>
        </dlentry>
        <dlentry id="GUID-E361BD3F-136B-4102-9B44-E188C42A9BEC">
          <dt>kmipServerFault</dt>
          <dd>
            <ul id="GUID-485282A0-326B-4AB7-8BD3-EBF32D4CD0D8">
              <li>Connection failure<p>To resolve this fault, check that the External Key Server is alive and reachable via the network. Use <cmdname>TestKeyServerKimp</cmdname> and <cmdname>TestKeyProviderKmip</cmdname> to test your connection.</p></li>
            </ul>
          </dd>
          <dd>
            <ul id="GUID-F42047FB-2BB0-42DB-854C-8772BDF17476">
              <li>Authentication failure<p>To resolve this fault, check that the correct root CA and KMIP client certificates are being used, and that the private key and the KMIP client certificate match.</p></li>
            </ul>
          </dd>
          <dd>
            <ul id="GUID-43938EEB-120A-49AA-90BF-256A4CD0A232">
              <li>Server error<p>To resolve this fault, check the details for the error. Troubleshooting on the External Key Server might be necessary based on the error returned.</p></li>
            </ul>
          </dd>
        </dlentry>
        <dlentry id="GUID-1490B06D-60E1-47B2-B351-A8A3BD5FA428">
          <dt>memoryEccThreshold</dt>
          <dd>A large number of correctable or uncorrectable ECC errors have been detected. This fault uses the following severities based on urgency:<simpletable frame="all" relcolwidth="1.56* 1.0* 1.23*"><sthead><stentry>Event</stentry><stentry>Severity</stentry><stentry>Description</stentry></sthead><strow><stentry>A single DIMM cErrorCount reaches cDimmCorrectableErrWarnThreshold.</stentry><stentry>Warning</stentry><stentry>Correctable ECC memory errors above threshold on DIMM: &lt;Processor&gt; &lt;DIMM Slot&gt;</stentry></strow><strow><stentry>A single DIMM cErrorCount stays above cDimmCorrectableErrWarnThreshold until cErrorFaultTimer expires for the DIMM.</stentry><stentry>Error</stentry><stentry>Correctable ECC memory errors above threshold on DIMM: &lt;Processor&gt; &lt;DIMM&gt;</stentry></strow><strow><stentry>A memory controller reports cErrorCount above cMemCtlrCorrectableErrWarnThreshold, and cMemCtlrCorrectableErrWarnDuration is specified.</stentry><stentry>Warning</stentry><stentry>Correctable ECC memory errors above threshold on memory controller: &lt;Processor&gt; &lt;Memory Controller&gt;</stentry></strow><strow><stentry>A memory controller reports cErrorCount above cMemCtlrCorrectableErrWarnThreshold until cErrorFaultTimer expires for the memory controller.</stentry><stentry>Error</stentry><stentry>Correctable ECC memory errors above threshold on DIMM: &lt;Processor&gt; &lt;DIMM&gt;</stentry></strow><strow><stentry>A single DIMM reports a uErrorCount above zero, but less than cDimmUncorrectableErrFaultThreshold.</stentry><stentry>Warning</stentry><stentry>Uncorrectable ECC memory error(s) detected on DIMM: &lt;Processor&gt; &lt;DIMM Slot&gt;</stentry></strow><strow><stentry>A single DIMM reports a uErrorCount of at least cDimmUncorrectableErrFaultThreshold.</stentry><stentry>Error</stentry><stentry>Uncorrectable ECC memory error(s) detected on DIMM: &lt;Processor&gt; &lt;DIMM Slot&gt;</stentry></strow><strow><stentry>A memory controller reports a uErrorCount above zero, but less than cMemCtlrUncorrectableErrFaultThreshold.</stentry><stentry>Warning</stentry><stentry>Uncorrectable ECC memory error(s) detected on memory controller: &lt;Processor&gt; &lt;Memory Controller&gt;</stentry></strow><strow><stentry>A memory controller reports a uErrorCount of at least cMemCtlrUncorrectableErrFaultThreshold.</stentry><stentry>Error</stentry><stentry>Uncorrectable ECC memory error(s) detected on memory controller: &lt;Processor&gt; &lt;Memory Controller&gt;</stentry></strow></simpletable></dd>
          <dd>To resolve this fault, contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-4C05DE00-5649-4649-B2E7-F10FE82AA54A">
          <dt>memoryUsageThreshold</dt>
          <dd>Memory usage is above normal. This fault uses the following severities based on urgency:<note id="GUID-C724BA52-821C-4416-ABFC-61D0CA423D85">See the <b>Details</b> heading in the error fault for more detailed information on the type of fault.</note><simpletable frame="all" relcolwidth="1.0* 1.0*"><sthead><stentry>Severity</stentry><stentry>Description</stentry></sthead><strow><stentry>Warning</stentry><stentry>System memory is low.</stentry></strow><strow><stentry>Error</stentry><stentry>System memory is very low. </stentry></strow><strow><stentry>Critical</stentry><stentry>System memory is completely consumed.</stentry></strow></simpletable></dd>
          <dd>To resolve this fault, contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-E86CA549-BB69-4A77-B460-5A7A38C532B0">
          <dt>metadataClusterFull</dt>
          <dd>There is not enough free metadata storage space to support a single node loss. See the <apiname>GetClusterFullThreshold</apiname> API method for details on cluster fullness levels. This cluster fault indicates one of the following conditions:<ul id="GUID-FB77A830-108D-4F37-ABF4-A9FD4F70C435"><li>stage3Low (Warning): User-defined threshold was crossed. Adjust Cluster Full settings or add more nodes.</li><li>stage4Critical (Error): There is not enough space to recover from a 1-node failure. Creation of volumes, snapshots, and clones is not allowed.</li><li>stage5CompletelyConsumed (Critical)1; No writes or new iSCSI connections are allowed. Current iSCSI connections will be maintained. Writes will fail until more capacity is added to the cluster. Purge or delete data or add more nodes.</li></ul></dd>
          <dd>To resolve this fault, purge or delete volumes or add another storage node to the storage cluster. </dd>
        </dlentry>
        <dlentry id="GUID-14274ED0-38A1-41DC-B8E6-D734EC6247B0">
          <dt>mtuCheckFailure</dt>
          <dd>A network device is not configured for the proper MTU size. </dd>
          <dd>To resolve this fault, ensure that all network interfaces and switch ports are configured for jumbo frames (MTUs up to 9000 bytes in size).</dd>
        </dlentry>
        <dlentry id="GUID-6A14E585-328D-44A0-986E-C04069A90F24">
          <dt>networkConfig</dt>
          <dd>This cluster fault indicates one of the following conditions:<ul id="UL_7392380BE008412DBE621588B2CC3733"><li id="LI_DABAFFF374914A52A3CCEBACB115FE1D">An expected interface is not present.</li><li id="LI_7649EB7614E5455A8AF806570B9DCD48">A duplicate interface is present.</li><li id="LI_85074FA2459248A3AC7B279B31F3F227">A configured interface is down.</li><li id="LI_73BEF9D90E064A79A3F11F931D2AC810">A network restart is required.</li></ul>Contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-3010DBD7-2164-4C5B-9547-112ED69FE2E0">
          <dt>noAvailableVirtualNetworkIPAddresses</dt>
          <dd>There are no available virtual network addresses in the block of IP addresses. <ul id="GUID-E6199E83-2548-4144-987D-17B315E72A46"><li>virtualNetworkID # TAG(###) has no available storage IP addresses. Additional nodes cannot be added to the cluster.</li></ul></dd>
          <dd>To resolve this fault, add more IP addresses to the block of virtual network addresses.</dd>
        </dlentry>
        <dlentry id="GUID-A94B67F9-1FD8-47D5-8539-F21733ABEC02">
          <dt>nodeHardwareFault (Network interface &lt;name&gt; is down or cable is unplugged)</dt>
          <dd>A network interface is either down or the cable is unplugged.</dd>
          <dd>To resolve this fault, check network connectivity for the node or nodes.</dd>
        </dlentry>
        <dlentry id="GUID-AFCF461F-3E0C-4CBC-A523-03F3BFC95DDD">
          <dt>nodeHardwareFault (Drive encryption capable state mismatches node's encryption capable state for the drive in slot &lt;node slot&gt;&lt;drive slot&gt;)</dt>
          <dd>A drive does not match encryption capabilities with the storage node it is installed in.</dd>
        </dlentry>
        <dlentry id="GUID-697184A0-6AF2-4458-A1B4-59C044A43269">
          <dt>nodeHardwareFault (Incorrect &lt;drive type&gt; drive size &lt;actual size&gt; for the drive in slot &lt;node slot&gt;&lt;drive slot&gt; for this node type - expected &lt;expected size&gt;)</dt>
          <dd>A storage node contains a drive that is the incorrect size for this node. </dd>
        </dlentry>
        <dlentry id="GUID-A1811F12-D773-42DA-9E66-5A89D8349575">
          <dt>nodeHardwareFault (Unsupported drive detected in slot &lt;node slot&gt;&lt;drive slot&gt;; drive statistics and health information will be unavailable)</dt>
          <dd>A storage node contains a drive it does not support.</dd>
        </dlentry>
        <dlentry id="GUID-632A6EBE-7047-445C-B36B-F425B2D2F8A6">
          <dt>nodeHardwareFault (The drive in slot &lt;node slot&gt;&lt;drive slot&gt; should be using firmware version &lt;expected version&gt;, but is using unsupported version &lt;actual version&gt;)</dt>
          <dd>A storage node contains a drive running an unsupported firmware version.</dd>
        </dlentry>
        <dlentry id="GUID-5AEC3ACF-6A6C-438C-A1D9-EECC2CB7ED16">
          <dt>nodeMaintenanceMode</dt>
          <dd>A node has been placed in maintenance mode. This fault uses the following severities based on urgency:<simpletable frame="all" relcolwidth="1.0* 1.0*"><sthead><stentry>Severity</stentry><stentry>Description</stentry></sthead><strow><stentry>Warning</stentry><stentry>Indicates that the node is still in maintenance mode.</stentry></strow><strow><stentry>Error</stentry><stentry>Indicates that maintenance mode has failed to disable, most likely due to failed or active standbys.</stentry></strow></simpletable></dd>
          <dd>To resolve this fault, disable maintenance mode once maintenance completes. If the Error level fault persists, contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-45D199E6-9518-4E6F-8905-E49805A6D5D3">
          <dt>nodeOffline</dt>
          <dd>Element software cannot communicate with the specified node. Check network connectivity.</dd>
        </dlentry>
        <dlentry id="GUID-4842A782-956E-4C08-85E2-730593C9AC44">
          <dt>notUsingLACPBondMode</dt>
          <dd>LACP bonding mode is not configured.   </dd>
          <dd>To resolve this fault, use LACP bonding when deploying storage nodes; clients might experience performance issues if LACP is not enabled and properly configured.</dd>
        </dlentry>
        <dlentry id="GUID-4EA6606C-55CD-402E-94E9-B42A70D5F846">
          <dt>ntpServerUnreachable</dt>
          <dd>The storage cluster cannot communicate with the specified NTP server or servers.</dd>
          <dd>To resolve this fault, check the configuration for the NTP server, network, and firewall. </dd>
        </dlentry>
        <dlentry id="GUID-CE26119F-E0DB-4909-B324-06A0983C697F">
          <dt>ntpTimeNotInSync</dt>
          <dd>The difference between storage cluster time and the specified NTP server time is too large. The storage cluster cannot correct the difference automatically.</dd>
          <dd>To resolve this fault, use NTP servers that are internal to your network, rather than the installation defaults. If you are using internal NTP servers and the issue persists, contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-EF9D16D6-88B2-452B-8FF5-DB7F0527410B">
          <dt>nvramDeviceStatus</dt>
          <dd>An NVRAM device has an error, is failing, or has failed. This fault has the following severities:<simpletable frame="all" relcolwidth="1.0* 1.0*"><sthead><stentry>Severity</stentry><stentry>Description</stentry></sthead><strow><stentry>Warning</stentry><stentry>A warning has been detected by the hardware. This condition may be transitory, such as a temperature warning. <ul id="GUID-C181BE4F-7E55-4B75-B142-C14013B70263"><li>nvmLifetimeError</li><li>nvmLifetimeStatus</li><li>energySourceLifetimeStatus</li><li>energySourceTemperatureStatus</li><li>warningThresholdExceeded</li></ul></stentry></strow><strow><stentry>Error</stentry><stentry>An Error or Critical status has been detected by the hardware. The cluster master attempts to remove the slice drive from operation (this generates a drive removal event). If secondary slice services are not available the drive will not be removed. Errors returned in addition to the Warning level errors:<ul id="GUID-CA83E1E5-4591-4195-BA12-EBE20B0CE5CC"><li>NVRAM device mount point doesn't exist.</li><li>NVRAM device partition doesn't exist.</li><li>NVRAM device partition exists, but not mounted.</li></ul></stentry></strow><strow><stentry>Critical</stentry><stentry>An Error or Critical status has been detected by the hardware. The cluster master attempts to remove the slice drive from operation (this generates a drive removal event). If secondary slice services are not available the drive will not be removed.<ul id="GUID-AA9A39E6-EF55-4FCD-84F1-6817BDE198E4"><li>persistenceLost</li><li>armStatusSaveNArmed</li><li>csaveStatusError</li></ul></stentry></strow></simpletable></dd>
          <dd>Replace any failed hardware in the node. If this does not resolve the issue, contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-203BBD58-F9EF-4C60-B7A1-78A0F9A80936">
          <dt>powerSupplyError</dt>
          <dd>This cluster fault indicates one of the following conditions:<ul id="UL_295B31885B534BC0BECCE2348CC32343"><li id="LI_FAD5BD647A004FB08AB72CBB158CD57E">A power supply is not present.</li><li id="LI_EF6BECA7013B4D3D8673933B74CBFBA7">A power supply has failed.</li><li id="LI_0896161A73C0437A91394325EB899E6A">A power supply input is missing or out of range.</li></ul><p>To resolve this fault, verify that redundant power is supplied to all nodes. Contact NetApp Support for assistance.</p></dd>
        </dlentry>
        <dlentry id="GUID-D8F4BEB2-E0AE-4EF7-A30E-F83344F669CF">
          <dt>provisionedSpaceTooFull</dt>
          <dd>The overall provisioned capacity of the cluster is too full.</dd>
          <dd>To resolve this fault, add more provisioned space, or delete and purge volumes.</dd>
        </dlentry>
        <dlentry id="GUID-588B09DC-DD93-4094-AA7B-B7A4AD9495EB">
          <dt>remoteRepAsyncDelayExceeded</dt>
          <dd>The configured asynchronous delay for replication has been exceeded. Check network connectivity between clusters.</dd>
        </dlentry>
        <dlentry id="GUID-4B22AF9B-7A16-40C4-9D83-EEF09F4CEEE9">
          <dt>remoteRepClusterFull</dt>
          <dd>The volumes have paused remote replication because the target storage cluster is too full.</dd>
          <dd>To resolve this fault, free up some space on the target storage cluster.</dd>
        </dlentry>
        <dlentry id="GUID-12DB31D1-34C6-41BC-AB58-922775BAB1C5">
          <dt>remoteRepSnapshotClusterFull</dt>
          <dd>The volumes have paused remote replication of snapshots because the target storage cluster is too full.</dd>
          <dd>To resolve this fault, free up some space on the target storage cluster.</dd>
        </dlentry>
        <dlentry id="GUID-531CDBD9-3E7B-472D-A9A7-2F03FAA12EB1">
          <dt>remoteRepSnapshotsExceededLimit</dt>
          <dd>The volumes have paused remote replication of snapshots because the target storage cluster volume has exceeded its snapshot limit. </dd>
          <dd>To resolve this fault, increase the snapshot limit on the target storage cluster.</dd>
        </dlentry>
        <dlentry id="GUID-8411AB7B-72D9-4DA7-8798-93E07DA86DB0">
          <dt>scheduleActionError</dt>
          <dd>One or more of the scheduled activities ran, but failed. </dd>
          <dd>The fault clears if the scheduled activity runs again and succeeds, if the scheduled activity is deleted, or if the activity is paused and resumed.</dd>
        </dlentry>
        <dlentry id="GUID-4CD109E6-C2A3-4FC1-993B-72717569E694">
          <dt>sensorReadingFailed </dt>
          <dd>The Baseboard Management Controller (BMC) self-test failed or a sensor could not communicate with the BMC.</dd>
          <dd>Contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-013D2D45-D8E8-49C6-813E-6720230A56E8">
          <dt>serviceNotRunning</dt>
          <dd>A required service is not running. </dd>
          <dd>Contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-6CFA65D3-7032-478C-95C7-5445B9B0458C">
          <dt>sliceServiceTooFull</dt>
          <dd>A slice service has too little provisioned capacity assigned to it. </dd>
          <dd>To resolve this fault, add more provisioned capacity.</dd>
        </dlentry>
        <dlentry id="GUID-18AFBE02-F8E4-4F2D-AE1A-1C4A88C0A92D">
          <dt>sliceServiceUnhealthy</dt>
          <dd>The system has detected that a slice service is unhealthy and is automatically decommissioning it. <ul id="GUID-142587E5-C12A-4B84-9B1B-02CA34CBD104"><li>Severity = Warning: No action is taken. This warning period will expire in 6 minutes.</li><li>Severity = Error: The system is automatically decommissioning data and re-replicating its data to other healthy drives.</li></ul>Check for network connectivity issues and hardware errors. There will be other faults if specific hardware components have failed. The fault will clear when the slice service is accessible or when the service has been decommissioned.</dd>
        </dlentry>
        <dlentry id="GUID-8A886BC1-EC28-443D-9117-60755DD26247">
          <dt>sshEnabled</dt>
          <dd>The SSH service is enabled on one or more nodes in the storage cluster.</dd>
          <dd> To resolve this fault, disable the SSH service on the appropriate node or nodes or contact NetApp Support for assistance. </dd>
        </dlentry>
        <dlentry id="GUID-C07F3D1F-0D49-43EB-B48D-E8BE11033860">
          <dt>sslCertificateExpiration</dt>
          <dd>The SSL certificate associated with this node is nearing expiration or has expired. This fault uses the following severities based on urgency: <simpletable frame="all" relcolwidth="1.0* 1.0*"><sthead><stentry>Severity</stentry><stentry>Description</stentry></sthead><strow><stentry>Warning</stentry><stentry>Certificate expires within 30 days.</stentry></strow><strow><stentry>Error</stentry><stentry>Certificate expires within 7 days.</stentry></strow><strow><stentry>Critical</stentry><stentry>Certificate expires within 3 days or has already expired.</stentry></strow></simpletable></dd>
          <dd>To resolve this fault, renew the SSL certificate. If needed, contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-D585E059-27D8-4BAD-918D-0D2F605FEB57">
          <dt>strandedCapacity</dt>
          <dd>A single node accounts for more than half of the storage cluster capacity. </dd>
          <dd>In order to maintain data redundancy, the system reduces the capacity of the largest node so that some of its block capacity is stranded (not used).</dd>
          <dd>To resolve this fault, add more drives to existing storage nodes or add storage nodes to the cluster.</dd>
        </dlentry>
        <dlentry id="GUID-7BC9C7AB-8D12-41B6-9BF0-B5A63B038D30">
          <dt>tempSensor</dt>
          <dd>A temperature sensor is reporting higher than normal temperatures. This fault can be triggered in conjunction with powerSupplyError or fanSensor faults.</dd>
          <dd>To resolve this fault, check for airflow obstructions near the storage cluster. If needed, contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-81F5231C-EC12-4684-8C23-E5B4F72AD16D">
          <dt>upgrade</dt>
          <dd>An upgrade has been in progress for more than 24 hours.</dd>
          <dd> to resolve this fault, resume the upgrade or contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-D3A00EA2-E5AD-4FFB-B5D7-4A8E965D4C5A">
          <dt>unresponsiveService</dt>
          <dd>A service has become unresponsive. </dd>
          <dd>Contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-6EE7BB1B-2B34-456C-A81F-89AC809B2695">
          <dt>virtualNetworkConfig</dt>
          <dd>This cluster fault indicates one of the following conditions:<ul id="UL_AB4FFCD4124F4B13ADB5F65A215D1096"><li id="LI_57F48753865C4F7EAC262467F2F7DE09">An interface is not present.</li><li id="LI_66F5C9741DAD4565979FE936AC7311A9">There is an incorrect namespace on an interface.</li><li id="LI_B13B43C5ABB64811AD298FCA22263068">There is an incorrect netmask.</li><li id="LI_82BBB4688CE5443A8F9CA159A5EB0229">There is an incorrect IP address.</li><li id="LI_B3E95D4EAE2A452A961426E117328FF8">An interface is not up and running.</li><li id="LI_98CE5612786D4D5280CE0FF1A22E8BD3">There is a superfluous interface on a  node.</li></ul>Contact NetApp Support for assistance.</dd>
        </dlentry>
        <dlentry id="GUID-7142679E-54B7-492D-9A8C-78443335A07C">
          <dt>volumesDegraded</dt>
          <dd>Secondary volumes have not finished replicating and synchronizing. The message is cleared when the synchronizing is complete.</dd>
        </dlentry>
        <dlentry id="GUID-7EDEC72F-A671-4324-99EB-9F71D104D442">
          <dt>volumesOffline</dt>
          <dd>One or more volumes in the storage cluster are offline. The <b>volumeDegraded</b> fault will also be present.</dd>
          <dd> Contact NetApp Support for assistance.</dd>
        </dlentry>
      </dl>
    </section>
    <section id="SECTION_0A932973431948D896C44E96EFF8CFF2">
      <dl>
        <dlentry id="GUID-DAD4BC65-2A40-4EFA-B97B-D967E8DE73A5">
          <dt>apiEvent</dt>
          <dd>Events initiated by a user through an API or web UI that modify settings.</dd>
        </dlentry>
        <dlentry id="GUID-8E9DE260-F43E-42A5-B10A-F7164ED4CE6A">
          <dt>binAssignmentsEvent</dt>
          <dd>Events related to the assignment of data bins. Bins are essentially containers that hold data and are mapped across the cluster.</dd>
        </dlentry>
        <dlentry id="GUID-DB99A37D-49CC-42EF-B4C6-126F7EC89D26">
          <dt>binSyncEvent</dt>
          <dd>System events related to a reassignment of data among block services.</dd>
        </dlentry>
        <dlentry id="GUID-76D666B1-F23E-48CC-B22C-776424DC271D">
          <dt>bsCheckEvent</dt>
          <dd>System events related to block service checks.</dd>
        </dlentry>
        <dlentry id="GUID-2BF4A445-402A-47D6-B301-BCB31D1F95A6">
          <dt>bsKillEvent</dt>
          <dd>System events related to block service terminations.</dd>
        </dlentry>
        <dlentry id="GUID-1AC7C922-29AF-44B5-B965-9F664F6BC51C">
          <dt>bulkOpEvent</dt>
          <dd>Events related to operations performed on an entire volume, such as a backup, restore, snapshot, or clone. </dd>
        </dlentry>
        <dlentry id="GUID-707A4A68-BDED-48C4-8930-67F51301FD03">
          <dt>cloneEvent</dt>
          <dd>Events related to volume cloning. </dd>
        </dlentry>
        <dlentry id="GUID-002C9E43-B93A-40DC-84B4-8F4F64C6957F">
          <dt>clusterMasterEvent</dt>
          <dd>Events appearing upon cluster initialization or upon configuration changes to the cluster, such as adding or removing nodes.</dd>
        </dlentry>
        <dlentry id="GUID-34EE1078-6BBF-47EE-ABB5-2E7B9B65F46E">
          <dt>csumEvent</dt>
          <dd>Events related to invalid data checksums on the disk.</dd>
        </dlentry>
        <dlentry id="GUID-000FDA83-4C7B-43EA-BDCF-25FC8CA31908">
          <dt>dataEvent</dt>
          <dd>Events related to reading and writing data.</dd>
        </dlentry>
        <dlentry id="GUID-3F4B72A6-FFC1-46B7-A0D3-ABF582E75EEB">
          <dt>dbEvent</dt>
          <dd>Events related to the global database maintained by ensemble nodes in the cluster.</dd>
        </dlentry>
        <dlentry id="GUID-925AF1CC-5B3E-488A-873C-32D26971FBD4">
          <dt>driveEvent</dt>
          <dd>Events related to drive operations.</dd>
        </dlentry>
        <dlentry id="GUID-C2539C13-7FAC-4889-9E81-0A1E4E5E53BB">
          <dt>encryptionAtRestEvent</dt>
          <dd>Events related to the process of encryption on a cluster.</dd>
        </dlentry>
        <dlentry id="GUID-E000DA2B-FB29-44DA-8EB1-13DA0F5B36BF">
          <dt>ensembleEvent</dt>
          <dd>Events related to increasing or decreasing the number of nodes in an ensemble. </dd>
        </dlentry>
        <dlentry id="GUID-998D6DE9-2B3F-46A0-888B-78DB8E5C54F5">
          <dt>fibreChannelEvent</dt>
          <dd>Events related to the configuration of and connections to the  nodes.</dd>
        </dlentry>
        <dlentry id="GUID-6E85FEB1-C467-4EE6-B02A-6BBD67FA6AAE">
          <dt>gcEvent</dt>
          <dd>Events related to processes run every 60 minutes to reclaim storage on block drives. This process is also known as garbage collection.</dd>
        </dlentry>
        <dlentry id="GUID-A4AD5D6A-3DC6-436C-95A9-5DE3279AB0CB">
          <dt>ieEvent</dt>
          <dd>Internal system error.</dd>
        </dlentry>
        <dlentry id="GUID-8D7E2B90-3D4E-410F-B5F8-85C320DF7852">
          <dt>installEvent</dt>
          <dd>Automatic software installation events. Software is being automatically installed on a pending node.</dd>
        </dlentry>
        <dlentry id="GUID-2A5CBCEE-6D05-46B6-A5C9-9AC30D4957B3">
          <dt>iSCSIEvent</dt>
          <dd>Events related to iSCSI issues in the system.</dd>
        </dlentry>
        <dlentry id="GUID-22A5364F-60BC-4A0F-8FFE-8A8D7B51DCD9">
          <dt>limitEvent</dt>
          <dd>Events related to the number of volumes or virtual volumes in an account or in the cluster nearing the maximum allowed.</dd>
        </dlentry>
        <dlentry id="GUID-8EE088F7-324F-4BD5-898E-C4653ED3AD86">
          <dt>maintenanceModeEvent</dt>
          <dd>Events related to the node maintenance mode, such as disabling the node.</dd>
        </dlentry>
        <dlentry id="GUID-6E725283-A671-49A8-B2C5-AD69BE7E69A1">
          <dt>networkEvent</dt>
          <dd>Events related to the status of virtual networking. </dd>
        </dlentry>
        <dlentry id="GUID-812C0A1B-FB03-49AF-83FF-467D1CFCC814">
          <dt>platformHardwareEvent</dt>
          <dd>Events related to issues detected on hardware devices.</dd>
        </dlentry>
        <dlentry id="GUID-8FA81090-63D8-49F0-A5B4-B5F79CD7F054">
          <dt>remoteClusterEvent</dt>
          <dd>Events related to remote cluster pairing.</dd>
        </dlentry>
        <dlentry id="GUID-21ED3744-BDCE-41CE-8CFC-CBE3F0C61ED5">
          <dt>schedulerEvent</dt>
          <dd>Events related to scheduled snapshots.</dd>
        </dlentry>
        <dlentry id="GUID-B9F7DD70-8BB6-4FBE-B3F1-C249087E02B7">
          <dt>serviceEvent</dt>
          <dd>Events related to system service status.</dd>
        </dlentry>
        <dlentry id="GUID-C46EA584-C5F8-4382-9223-D559CC18C26F">
          <dt>sliceEvent</dt>
          <dd>Events related to the Slice Server, such as removing a metadata drive or
                            volume.<p>There are three types of
                            slice reassignment events, which include information about the service
                            where a volume is assigned: <ul id="GUID-892646F3-9F2C-4D0B-BD6D-17DE60FD8635"><li>flipping: changing the primary service to a new
                                    primary
                                    service<screen xml:space="preserve">sliceID oldPrimaryServiceID-&gt;newPrimaryServiceID</screen></li><li>moving: changing the secondary service to a new
                                    secondary
                                    service<screen xml:space="preserve">sliceID {oldSecondaryServiceID(s)}-&gt;{newSecondaryServiceID(s)}</screen></li><li>pruning: removing a volume from a set of
                                    services<screen xml:space="preserve">sliceID {oldSecondaryServiceID(s)}</screen></li></ul></p></dd>
        </dlentry>
        <dlentry id="GUID-DE544D5A-CB1E-4256-AC3F-3FE76B690A21">
          <dt>snmpTrapEvent</dt>
          <dd>Events related to SNMP traps.</dd>
        </dlentry>
        <dlentry id="GUID-5D701A48-CEDA-47C0-9C94-AA9F2D39EF93">
          <dt>statEvent</dt>
          <dd>Events related to system statistics.</dd>
        </dlentry>
        <dlentry id="GUID-C623D09A-AAD3-475A-99A1-96BCD8E2C3AE">
          <dt>tsEvent</dt>
          <dd>Events related to the system transport service.</dd>
        </dlentry>
        <dlentry id="GUID-2B54ADBB-5146-4C6F-992D-E0CD3781198D">
          <dt>unexpectedException</dt>
          <dd>Events related to unexpected system exceptions.</dd>
        </dlentry>
        <dlentry id="GUID-AD991A57-2C16-4495-8870-E0080CA925D8">
          <dt>ureEvent</dt>
          <dd>Events related to Unrecoverable Read Errors that occur while reading from the storage device.</dd>
        </dlentry>
        <dlentry id="GUID-D2574EEB-5FE9-47DB-A86A-608ABD97AE18">
          <dt>vasaProviderEvent</dt>
          <dd>Events related to a VASA (vSphere APIs for Storage Awareness) Provider.</dd>
        </dlentry>
      </dl>
    </section>
  </refbody>
</reference>